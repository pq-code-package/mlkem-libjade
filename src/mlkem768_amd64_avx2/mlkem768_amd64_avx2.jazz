param int MLKEM_K = 3;

param int MLKEM_Q = 3329;
param int MLKEM_N = 256;
param int MLKEM_VECN = MLKEM_K * MLKEM_N;

param int MLKEM_SYMBYTES = 32;
param int MLKEM_SSBYTES = 32;

param int MLKEM_ETA1 = 2;
param int MLKEM_ETA2 = 2;

param int MLKEM_POLYBYTES = 384;
param int MLKEM_POLYVECBYTES = (MLKEM_K * MLKEM_POLYBYTES);

param int MLKEM_POLYCOMPRESSEDBYTES = 128;
param int MLKEM_POLYVECCOMPRESSEDBYTES = (MLKEM_K * 320);

param int MLKEM_INDCPA_MSGBYTES = MLKEM_SYMBYTES;
param int MLKEM_INDCPA_PUBLICKEYBYTES = MLKEM_POLYVECBYTES + MLKEM_SYMBYTES;
param int MLKEM_INDCPA_SECRETKEYBYTES = MLKEM_POLYVECBYTES;
param int MLKEM_INDCPA_CIPHERTEXTBYTES = MLKEM_POLYVECCOMPRESSEDBYTES + MLKEM_POLYCOMPRESSEDBYTES;

param int MLKEM_PUBLICKEYBYTES = MLKEM_INDCPA_PUBLICKEYBYTES;
param int MLKEM_SECRETKEYBYTES = MLKEM_INDCPA_SECRETKEYBYTES + MLKEM_INDCPA_PUBLICKEYBYTES + 2*MLKEM_SYMBYTES;
param int MLKEM_CIPHERTEXTBYTES = MLKEM_INDCPA_CIPHERTEXTBYTES;
inline 
fn __shuffle8(reg u256 a b) -> reg u256, reg u256
{
  reg u256 r0 r1; 
  r0 = #VPERM2I128(a,b,0x20);
  r1 = #VPERM2I128(a,b,0x31);
  return r0, r1;
}

inline 
fn __shuffle4(reg u256 a b) -> reg u256, reg u256
{
  reg u256 r0 r1; 
  r0 = #VPUNPCKL_4u64(a,b);
  r1 = #VPUNPCKH_4u64(a,b);
  return r0, r1;
}

inline 
fn __shuffle2(reg u256 a b) -> reg u256, reg u256
{
  reg u256 t0 t1;
  t0 = #VMOVSLDUP_256(b);
  t0 = #VPBLEND_8u32(a, t0, 0xAA);
  a = #VPSRL_4u64(a,32);
  t1 = #VPBLEND_8u32(a, b, 0xAA);
  return t0, t1;
}


inline 
fn __shuffle1(reg u256 a b) -> reg u256, reg u256
{
  reg u256 r0 r1 t0 t1; 
  t0 = #VPSLL_8u32(b,16);
  r0 = #VPBLEND_16u16(a,t0,0xAA);
  t1 = #VPSRL_8u32(a,16);
  r1 = #VPBLEND_16u16(t1,b,0xAA);
  return r0, r1;
}


// Transform from AVX order to bitreversed order
inline 
fn __nttpack128(reg u256 r0 r1 r2 r3 r4 r5 r6 r7)
    -> reg u256, reg u256, reg u256, reg u256, reg u256, reg u256, reg u256, reg u256
{
  r0, r1 = __shuffle1(r0, r1);
  r2, r3 = __shuffle1(r2, r3);
  r4, r5 = __shuffle1(r4, r5);
  r6, r7 = __shuffle1(r6, r7);

  r0, r2 = __shuffle2(r0, r2);
  r4, r6 = __shuffle2(r4, r6);
  r1, r3 = __shuffle2(r1, r3);
  r5, r7 = __shuffle2(r5, r7);

  r0, r4 = __shuffle4(r0, r4);
  r1, r5 = __shuffle4(r1, r5);
  r2, r6 = __shuffle4(r2, r6);
  r3, r7 = __shuffle4(r3, r7);

  r0, r1 = __shuffle8(r0, r1);
  r2, r3 = __shuffle8(r2, r3);
  r4, r5 = __shuffle8(r4, r5);
  r6, r7 = __shuffle8(r6, r7);

  return r0, r2, r4, r6, r1, r3, r5, r7;
}


// Transform from bitreversed order to AVX order
inline
fn __nttunpack128(reg u256 r0 r1 r2 r3 r4 r5 r6 r7)
    -> reg u256, reg u256, reg u256, reg u256, reg u256, reg u256, reg u256, reg u256
{
  r0, r4 = __shuffle8(r0, r4);
  r1, r5 = __shuffle8(r1, r5);
  r2, r6 = __shuffle8(r2, r6);
  r3, r7 = __shuffle8(r3, r7);

  r0, r2 = __shuffle4(r0, r2);
  r4, r6 = __shuffle4(r4, r6);
  r1, r3 = __shuffle4(r1, r3);
  r5, r7 = __shuffle4(r5, r7);

  r0, r1 = __shuffle2(r0, r1);
  r2, r3 = __shuffle2(r2, r3);
  r4, r5 = __shuffle2(r4, r5);
  r6, r7 = __shuffle2(r6, r7);

  r0, r4 = __shuffle1(r0, r4);
  r1, r5 = __shuffle1(r1, r5);
  r2, r6 = __shuffle1(r2, r6);
  r3, r7 = __shuffle1(r3, r7);

  return r0, r4, r1, r5, r2, r6, r3, r7;
}

fn _nttpack(reg ptr u16[MLKEM_N] rp) -> reg ptr u16[MLKEM_N]
{
  reg u256 r0 r1 r2 r3 r4 r5 r6 r7;

  r0 = rp.[u256 32*0];
  r1 = rp.[u256 32*1];
  r2 = rp.[u256 32*2];
  r3 = rp.[u256 32*3];
  r4 = rp.[u256 32*4];
  r5 = rp.[u256 32*5];
  r6 = rp.[u256 32*6];
  r7 = rp.[u256 32*7];

  r0, r1, r2, r3, r4, r5, r6, r7 = __nttpack128(r0, r1, r2, r3, r4, r5, r6, r7);

  rp.[u256 32*0] = r0;
  rp.[u256 32*1] = r1;
  rp.[u256 32*2] = r2;
  rp.[u256 32*3] = r3;
  rp.[u256 32*4] = r4;
  rp.[u256 32*5] = r5;
  rp.[u256 32*6] = r6;
  rp.[u256 32*7] = r7;

  r0 = rp.[u256 32*8];
  r1 = rp.[u256 32*9];
  r2 = rp.[u256 32*10];
  r3 = rp.[u256 32*11];
  r4 = rp.[u256 32*12];
  r5 = rp.[u256 32*13];
  r6 = rp.[u256 32*14];
  r7 = rp.[u256 32*15];

  r0, r1, r2, r3, r4, r5, r6, r7 = __nttpack128(r0, r1, r2, r3, r4, r5, r6, r7);

  rp.[u256 32*8] = r0;
  rp.[u256 32*9] = r1;
  rp.[u256 32*10] = r2;
  rp.[u256 32*11] = r3;
  rp.[u256 32*12] = r4;
  rp.[u256 32*13] = r5;
  rp.[u256 32*14] = r6;
  rp.[u256 32*15] = r7;

  return rp;
}

fn _nttunpack(reg ptr u16[MLKEM_N] rp) -> reg ptr u16[MLKEM_N]
{
  reg u256 r0 r1 r2 r3 r4 r5 r6 r7;

  r0 = rp.[u256 32*0];
  r1 = rp.[u256 32*1];
  r2 = rp.[u256 32*2];
  r3 = rp.[u256 32*3];
  r4 = rp.[u256 32*4];
  r5 = rp.[u256 32*5];
  r6 = rp.[u256 32*6];
  r7 = rp.[u256 32*7];

  r0, r1, r2, r3, r4, r5, r6, r7 = __nttunpack128(r0, r1, r2, r3, r4, r5, r6, r7);

  rp.[u256 32*0] = r0;
  rp.[u256 32*1] = r1;
  rp.[u256 32*2] = r2;
  rp.[u256 32*3] = r3;
  rp.[u256 32*4] = r4;
  rp.[u256 32*5] = r5;
  rp.[u256 32*6] = r6;
  rp.[u256 32*7] = r7;

  r0 = rp.[u256 32*8];
  r1 = rp.[u256 32*9];
  r2 = rp.[u256 32*10];
  r3 = rp.[u256 32*11];
  r4 = rp.[u256 32*12];
  r5 = rp.[u256 32*13];
  r6 = rp.[u256 32*14];
  r7 = rp.[u256 32*15];

  r0, r1, r2, r3, r4, r5, r6, r7 = __nttunpack128(r0, r1, r2, r3, r4, r5, r6, r7);

  rp.[u256 32*8] = r0;
  rp.[u256 32*9] = r1;
  rp.[u256 32*10] = r2;
  rp.[u256 32*11] = r3;
  rp.[u256 32*12] = r4;
  rp.[u256 32*13] = r5;
  rp.[u256 32*14] = r6;
  rp.[u256 32*15] = r7;

  return rp;
}
u16[128] jzetas = {2285, 2571, 2970, 1812, 1493, 1422, 287, 202, 3158, 622, 1577, 182, 962, 2127, 1855, 1468, 
                  573, 2004, 264, 383, 2500, 1458, 1727, 3199, 2648, 1017, 732, 608, 1787, 411, 3124, 1758, 
                  1223, 652, 2777, 1015, 2036, 1491, 3047, 1785, 516, 3321, 3009, 2663, 1711, 2167, 126, 1469, 
                  2476, 3239, 3058, 830, 107, 1908, 3082, 2378, 2931, 961, 1821, 2604, 448, 2264, 677, 2054, 
                  2226, 430, 555, 843, 2078, 871, 1550, 105, 422, 587, 177, 3094, 3038, 2869, 1574, 1653, 
                  3083, 778, 1159, 3182, 2552, 1483, 2727, 1119, 1739, 644, 2457, 349, 418, 329, 3173, 3254, 
                  817, 1097, 603, 610, 1322, 2044, 1864, 384, 2114, 3193, 1218, 1994, 2455, 220, 2142, 1670, 
                  2144, 1799, 2051, 794, 1819, 2475, 2459, 478, 3221, 3021, 996, 991, 958, 1869, 1522, 1628};


u16[128] jzetas_inv = {1701, 1807, 1460, 2371, 2338, 2333, 308, 108, 2851, 870, 854, 1510, 2535, 1278, 1530, 1185, 
                       1659, 1187, 3109, 874, 1335, 2111, 136, 1215, 2945, 1465, 1285, 2007, 2719, 2726, 2232, 2512, 
                       75, 156, 3000, 2911, 2980, 872, 2685, 1590, 2210, 602, 1846, 777, 147, 2170, 2551, 246, 
                       1676, 1755, 460, 291, 235, 3152, 2742, 2907, 3224, 1779, 2458, 1251, 2486, 2774, 2899, 1103, 
                       1275, 2652, 1065, 2881, 725, 1508, 2368, 398, 951, 247, 1421, 3222, 2499, 271, 90, 853, 
                       1860, 3203, 1162, 1618, 666, 320, 8, 2813, 1544, 282, 1838, 1293, 2314, 552, 2677, 2106, 
                       1571, 205, 2918, 1542, 2721, 2597, 2312, 681, 130, 1602, 1871, 829, 2946, 3065, 1325, 2756, 
                       1861, 1474, 1202, 2367, 3147, 1752, 2707, 171, 3127, 3042, 1907, 1836, 1517, 359, 758, 1441};

u16[400] jzetas_exp = {31499, 31499,  2571,  2571, 14746, 14746,  2970,  2970, 13525, 13525, 13525, 13525, 13525, 13525, 13525, 13525,
                       53134, 53134, 53134, 53134, 53134, 53134, 53134, 53134, 1493,  1493,  1493,  1493,  1493,  1493,  1493,  1493,
                        1422,  1422,  1422,  1422,  1422,  1422,  1422,  1422, 44630, 44630, 44630, 44630, 27758, 27758, 27758, 27758,
                       61737, 61737, 61737, 61737, 49846, 49846, 49846, 49846, 3158,  3158,  3158,  3158,   622,   622,   622,   622,
                        1577,  1577,  1577,  1577,   182,   182,   182,   182, 59709, 59709, 17364, 17364, 39176, 39176, 36479, 36479,
                        5572,  5572, 64434, 64434, 21439, 21439, 39295, 39295, 573,   573,  2004,  2004,   264,   264,   383,   383,
                        2500,  2500,  1458,  1458,  1727,  1727,  3199,  3199, 59847, 59020,  1497, 30967, 41972, 20179, 20711, 25081,
                       52740, 26617, 16065, 53095,  9135, 64887, 39550, 27837, 1223,   652,  2777,  1015,  2036,  1491,  3047,  1785,
                         516,  3321,  3009,  2663,  1711,  2167,   126,  1469, 65202, 54059, 33310, 20494, 37798,   945, 50654,  6182,
                       32011, 10631, 29176, 36775, 47051, 17561, 51106, 60261, 2226,   555,  2078,  1550,   422,   177,  3038,  1574,
                        3083,  1159,  2552,  2727,  1739,  2457,   418,  3173, 11182, 13387, 51303, 43881, 13131, 60950, 23093,  5493,
                       33034, 30318, 46795, 12639, 20100, 18525, 19529, 52918, 430,   843,   871,   105,   587,  3094,  2869,  1653,
                         778,  3182,  1483,  1119,   644,   349,   329,  3254, 788,   788,  1812,  1812, 28191, 28191, 28191, 28191,
                       28191, 28191, 28191, 28191, 48842, 48842, 48842, 48842, 48842, 48842, 48842, 48842,   287,   287,   287,   287,
                         287,   287,   287,   287,   202,   202,   202,   202, 202,   202,   202,   202, 10690, 10690, 10690, 10690,
                        1359,  1359,  1359,  1359, 54335, 54335, 54335, 54335, 31164, 31164, 31164, 31164,   962,   962,   962,   962,
                        2127,  2127,  2127,  2127,  1855,  1855,  1855,  1855, 1468,  1468,  1468,  1468, 37464, 37464, 24313, 24313,
                       55004, 55004,  8800,  8800, 18427, 18427,  8859,  8859, 26676, 26676, 49374, 49374,  2648,  2648,  1017,  1017,
                         732,   732,   608,   608,  1787,  1787,   411,   411, 3124,  3124,  1758,  1758, 19884, 37287, 49650, 56638,
                       37227,  9076, 35338, 18250, 13427, 14017, 36381, 52780, 16832,  4312, 41381, 47622,  2476,  3239,  3058,   830,
                         107,  1908,  3082,  2378,  2931,   961,  1821,  2604, 448,  2264,   677,  2054, 34353, 25435, 58154, 24392,
                       44610, 10946, 24215, 16990, 10336, 57603, 43035, 10907, 31637, 28644, 23998, 48114,   817,   603,  1322,  1864,
                        2114,  1218,  2455,  2142,  2144,  2051,  1819,  2459, 3221,   996,   958,  1522, 20297,  2146, 15356, 33152,
                       59257, 50634, 54492, 14470, 44039, 45338, 23211, 48094, 41677, 45279,  7757, 23132,  1097,   610,  2044,   384,
                        3193,  1994,   220,  1670,  1799,   794,  2475,   478, 3021,   991,  1869,  1628,     0,     0,     0,     0};

u16[400] jzetas_inv_exp = {42405, 57780, 20258, 23860, 17443, 42326, 20199, 21498, 51067, 11045, 14903,  6280, 32385, 50181, 63391, 45240,
                            1701,  1460,  2338,   308,  2851,   854,  2535,  1530, 1659,  3109,  1335,   136,  2945,  1285,  2719,  2232,
                           17423, 41539, 36893, 33900, 54630, 22502,  7934, 55201, 48547, 41322, 54591, 20927, 41145,  7383, 40102, 31184,
                            1807,  2371,  2333,   108,   870,  1510,  1278,  1185, 1187,   874,  2111,  1215,  1465,  2007,  2726,  2512,
                           17915, 24156, 61225, 48705, 12757, 29156, 51520, 52110, 47287, 30199, 56461, 28310,  8899, 15887, 28250, 45653,
                            1275,  2652,  1065,  2881,   725,  1508,  2368,   398, 951,   247,  1421,  3222,  2499,   271,    90,   853,
                           16163, 16163, 38861, 38861, 56678, 56678, 47110, 47110, 56737, 56737, 10533, 10533, 41224, 41224, 28073, 28073,
                            1571,  1571,   205,   205,  2918,  2918,  1542,  1542, 2721,  2721,  2597,  2597,  2312,  2312,   681,   681,
                           34373, 34373, 34373, 34373, 11202, 11202, 11202, 11202, 64178, 64178, 64178, 64178, 54847, 54847, 54847, 54847,
                            1861,  1861,  1861,  1861,  1474,  1474,  1474,  1474, 1202,  1202,  1202,  1202,  2367,  2367,  2367,  2367,
                           16695, 16695, 16695, 16695, 16695, 16695, 16695, 16695, 37346, 37346, 37346, 37346, 37346, 37346, 37346, 37346,
                            3127,  3127,  3127,  3127,  3127,  3127,  3127,  3127, 3042,  3042,  3042,  3042,  3042,  3042,  3042,  3042,
                           64749, 64749,  1517,  1517, 12619, 46008, 47012, 45437, 52898, 18742, 35219, 32503, 60044, 42444,  4587, 52406,
                           21656, 14234, 52150, 54355,    75,  3000,  2980,  2685, 2210,  1846,   147,  2551,  1676,   460,   235,  2742,
                            3224,  2458,  2486,  2899,  5276, 14431, 47976, 18486, 28762, 36361, 54906, 33526, 59355, 14883, 64592, 27739,
                           45043, 32227, 11478,   335,   156,  2911,   872,  1590, 602,   777,  2170,   246,  1755,   291,  3152,  2907,
                            1779,  1251,  2774,  1103, 37700, 25987,   650, 56402, 12442, 49472, 38920, 12797, 40456, 44826, 45358, 23565,
                           34570, 64040,  6517,  5690,  1860,  3203,  1162,  1618, 666,   320,     8,  2813,  1544,   282,  1838,  1293,
                            2314,   552,  2677,  2106, 26242, 26242, 44098, 44098, 1103,  1103, 59965, 59965, 29058, 29058, 26361, 26361,
                           48173, 48173,  5828,  5828,   130,   130,  1602,  1602, 1871,  1871,   829,   829,  2946,  2946,  3065,  3065,
                            1325,  1325,  2756,  2756, 15691, 15691, 15691, 15691, 3800,  3800,  3800,  3800, 37779, 37779, 37779, 37779,
                           20907, 20907, 20907, 20907,  3147,  3147,  3147,  3147, 1752,  1752,  1752,  1752,  2707,  2707,  2707,  2707,
                             171,   171,   171,   171, 12403, 12403, 12403, 12403, 12403, 12403, 12403, 12403, 52012, 52012, 52012, 52012,
                           52012, 52012, 52012, 52012,  1907,  1907,  1907,  1907, 1907,  1907,  1907,  1907,  1836,  1836,  1836,  1836,
                            1836,  1836,  1836,  1836, 50791, 50791,   359,   359, 60300, 60300,  1932,  1932,     0,     0,     0,     0
};

u16[16] jqx16 = {MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q,
                 MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q, MLKEM_Q};

u16[16] jqinvx16 = {62209, 62209, 62209, 62209, 62209, 62209, 62209, 62209, 
                    62209, 62209, 62209, 62209, 62209, 62209, 62209, 62209};

u16[16] jvx16 = {20159, 20159, 20159, 20159, 20159, 20159, 20159, 20159,
                 20159, 20159, 20159, 20159, 20159, 20159, 20159, 20159};

u16[16] jfhix16 = {1441, 1441, 1441, 1441, 1441, 1441, 1441, 1441,
                   1441, 1441, 1441, 1441, 1441, 1441, 1441, 1441};

u16[16] jflox16 = {55457, 55457, 55457, 55457, 55457, 55457, 55457, 55457,
                   55457, 55457, 55457, 55457, 55457, 55457, 55457, 55457};

u16[16] maskx16 = {4095, 4095, 4095, 4095, 4095, 4095, 4095, 4095,
                   4095, 4095, 4095, 4095, 4095, 4095, 4095, 4095};

u16[16] hqx16_p1 = {1665, 1665, 1665, 1665, 1665, 1665, 1665, 1665,
                 1665, 1665, 1665, 1665, 1665, 1665, 1665, 1665};

u16[16] hqx16_m1 =  {1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664,
                 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664};

u16[16] hhqx16 = {832, 832, 832, 832, 832, 832, 832, 832,
                  832, 832, 832, 832, 832, 832, 832, 832};

u16[16] mqinvx16 = {80635, 80635, 80635, 80635, 80635, 80635, 80635, 80635,
                    80635, 80635, 80635, 80635, 80635, 80635, 80635, 80635};

u16[16] jdmontx16 = {1353, 1353, 1353, 1353, 1353, 1353, 1353, 1353,
                    1353, 1353, 1353, 1353, 1353, 1353, 1353, 1353};

param int QINV    = 62209;     /* q^(-1) mod 2^16 */
param int MONT    = 2285;      /* 2^16 % Q */
param int BARR    = 20159;     /* (1U << 26)/MLKEM_Q + 1 */

inline 
fn __csubq(reg u256 r qx16) -> reg u256
{
  reg u256 t;
  r = #VPSUB_16u16(r, qx16);
  t = #VPSRA_16u16(r, 15);
  t = #VPAND_256(t, qx16);
  r = #VPADD_16u16(t, r);
  return r;
}

inline 
fn __red16x(reg u256 r qx16 vx16) -> reg u256
{
  reg u256 x;
  x = #VPMULH_16u16(r, vx16);
  x = #VPSRA_16u16(x, 10);
  x = #VPMULL_16u16(x, qx16);
  r = #VPSUB_16u16(r, x);
  return r;
}

inline 
fn __fqmulprecomp16x(reg u256 b al ah qx16) -> reg u256
{
  reg u256 x;
  x = #VPMULL_16u16(al, b);
  b = #VPMULH_16u16(ah, b);
  x = #VPMULH_16u16(x, qx16);
  b = #VPSUB_16u16(b, x);
  return b;
}

inline
fn __fqmulx16(reg u256 a b qx16 qinvx16) -> reg u256
{
  reg u256 rd rhi rlo;
  rhi = #VPMULH_16u16(a, b);
  rlo = #VPMULL_16u16(a, b);

  rlo = #VPMULL_16u16(rlo, qinvx16);
  rlo = #VPMULH_16u16(rlo, qx16);
  rd = #VPSUB_16u16(rhi, rlo);

  return rd;
}

inline
fn __fqmul(reg u16 a, reg u16 b) -> reg u16
{
  reg u32 ad;
  reg u32 bd;
  reg u32 c;
  reg u32 t;
  reg u16 r;
  reg u32 u;

  ad = (32s)a;
  bd = (32s)b;

  c = ad * bd;

  u = c * QINV;
  u <<= 16;
  //u = #SAR_32(u, 16);
  u >>s= 16;
  t = u * MLKEM_Q;
  t = c - t;
  //t = #SAR_32(t, 16);
  t >>s= 16;
  r = t;
  return r;
}

inline
fn __barrett_reduce(reg u16 a) -> reg u16
{
  reg u32 t;
  reg u16 r;
  t = (32s)a;
  t = t * BARR;
  //t = #SAR_32(t, 26);
  t >>s= 26;
  t *= MLKEM_Q;
  r = t;
  r = a;
  r -= t;
  return r;
}
param int KECCAK_ROUNDS = 24;

u64[24] KECCAK1600_RC =
{  0x0000000000000001
  ,0x0000000000008082
  ,0x800000000000808a
  ,0x8000000080008000
  ,0x000000000000808b
  ,0x0000000080000001
  ,0x8000000080008081
  ,0x8000000000008009
  ,0x000000000000008a
  ,0x0000000000000088
  ,0x0000000080008009
  ,0x000000008000000a
  ,0x000000008000808b
  ,0x800000000000008b
  ,0x8000000000008089
  ,0x8000000000008003
  ,0x8000000000008002
  ,0x8000000000000080
  ,0x000000000000800a
  ,0x800000008000000a
  ,0x8000000080008081
  ,0x8000000000008080
  ,0x0000000080000001
  ,0x8000000080008008
};

inline fn keccakf1600_index(inline int x y) -> inline int
{
  inline int r;
  r = (x % 5) + 5 * (y % 5);
  return r;
}

inline fn keccakf1600_rho_offsets(inline int i) -> inline int
{
  inline int r x y z t;

  r = 0;
  x = 1;
  y = 0;

  for t = 0 to 24
  { if (i == x + 5 * y)
    { r = ((t + 1) * (t + 2) / 2) % 64; }
    z = (2 * x + 3 * y) % 5;
    x = y;
    y = z;
  }

  return r;
}

inline fn keccakf1600_rhotates(inline int x y) -> inline int
{
  inline int i r;
  i = keccakf1600_index(x, y);
  r = keccakf1600_rho_offsets(i);
  return r;
}

// C[x] = A[x,0] ^ A[x,1] ^ A[x,2] ^ A[x,3] ^ A[x,4]
inline fn keccakf1600_theta_sum(reg ptr u64[25] a) -> reg u64[5]
{
  inline int x y;
  reg u64[5] c;

  // C[x] = A[x, 0]
  for x=0 to 5
  { c[x] = a[x + 0]; }

  // C[x] ^= A[x,1] ^ A[x,2] ^ A[x,3] ^ A[x,4]
  for y=1 to 5
  { for x=0 to 5
    { c[x] ^= a[x + y*5]; }
  }

  return c;
}

// D[x] = C[x-1] ^ ROT(C[x+1], 1)
inline fn keccakf1600_theta_rol(reg u64[5] c) -> reg u64[5]
{
  inline int x;
  reg u64[5] d;

  for x = 0 to 5
  { // D[x] = C[x + 1]
    d[x] = c[(x + 1) % 5];

    // D[x] = ROT(D[x], 1)
    _, _, d[x] = #ROL_64(d[x], 1);

    // D[x] ^= C[x-1]
    d[x] ^= c[(x - 1 + 5) % 5];
  }

  return d;
}

// B[x] = ROT( (A[x',y'] ^ D[x']), r[x',y'] ) with (x',y') = M^-1 (x,y)
//
// M = (0 1)  M^-1 = (1 3)  x' = 1x + 3y
//     (2 3)         (1 0)  y' = 1x + 0y
//
inline fn keccakf1600_rol_sum(
  reg ptr u64[25] a,
  reg u64[5] d,
  inline int y)
  ->
  reg u64[5]
{
  inline int r x x_ y_;
  reg u64[5] b;

  for x = 0 to 5
  {
    x_ = (x + 3*y) % 5;
    y_ = x;
    r = keccakf1600_rhotates(x_, y_);

    // B[x] = A[x',y']
    b[x] = a[x_ + y_*5];

    // B[x] ^= D[x'];
    b[x] ^= d[x_];

    // B[x] = ROT( B[x], r[x',y'] );
    if(r != 0)
    { _, _, b[x] = #ROL_64(b[x], r); }

  }

  return b;
}

// E[x, y] = B[x] ^ ( (!B[x+1]) & B[x+2] )
// -- when x and y are 0: E[0,0] ^= RC[i];
inline fn keccakf1600_set_row(
  reg ptr u64[25] e,
  reg u64[5] b,
  inline int y,
  stack u64 s_rc)
  ->
  reg ptr u64[25]
{
  inline int x x1 x2;
  reg u64 t;

  for x=0 to 5
  {
    x1 = (x + 1) % 5;
    x2 = (x + 2) % 5;

    t  = !b[x1] & b[x2]; // bmi1
    //t = b[x1]; t = !t; t &= b[x2];

    t ^= b[x];
    if( x==0 && y==0 ){ t ^= s_rc; }
    e[x + y*5] = t;
  }

  return e;
}

inline fn keccakf1600_round(
  reg ptr u64[25] e,
  reg ptr u64[25] a,
  reg u64 rc)
  ->
  reg ptr u64[25]
{
  inline int y;
  reg u64[5] b c d;
  stack u64 s_rc;

  s_rc = rc;

  c = keccakf1600_theta_sum(a);
  d = keccakf1600_theta_rol(c);

  for y = 0 to 5
  { b = keccakf1600_rol_sum(a, d, y);
    e = keccakf1600_set_row(e, b, y, s_rc);
  }

  return e;
}

inline fn __keccakf1600(reg ptr u64[25] a) -> reg ptr u64[25]
{
  reg ptr u64[24] RC;
  stack u64[25] s_e;
  reg ptr u64[25] e;

  reg u64 c rc;

  RC = KECCAK1600_RC;
  e = s_e;

  c = 0;
  while (c < KECCAK_ROUNDS - 1)
  {
    rc = RC[(int) c];
    e = keccakf1600_round(e, a, rc);

    rc = RC[(int) c + 1];
    a = keccakf1600_round(a, e, rc);

    c += 2;
  }

  return a;
}

fn _keccakf1600(reg ptr u64[25] a) -> reg ptr u64[25]
{
  a = __keccakf1600(a);
  return a;
}

inline fn _keccakf1600_(reg ptr u64[25] a) -> reg ptr u64[25]
{
  a = a;
  a = _keccakf1600(a);
  a = a;
  return a;
}
param int SHAKE128_RATE = 168;
param int SHAKE256_RATE = 136;
param int SHA3_256_RATE = 136;
param int SHA3_512_RATE = 72;

u64[4] shake_sep = {9223372036854775808, 9223372036854775808, 9223372036854775808, 9223372036854775808};

inline
fn __st0(reg ptr u64[25] state) -> reg ptr u64[25]
{
  inline int i;

  for i = 0 to 25 {
    state[i] = 0;
  }

  return state;
}


inline
fn __add_full_block(
  stack u64[25] state,
  reg u64 in,
  reg u64 inlen,
  reg u64 r8
) -> stack u64[25], reg u64, reg u64
{
  reg u64 i t r64;

  r64 = r8;
  r64 >>= 3;
  i = 0;
  while (i < r64)
  {
    t = [in + 8 * i];
    state[(int) i] ^= t;
    i = i + 1;
  }

  in += r8;
  inlen -= r8;

  return state, in, inlen;
}


inline
fn __add_final_block(
  stack u64[25] state,
  reg u64 in,
  reg u64 inlen,
  reg u8 trail_byte,
  reg u64 r8
) -> stack u64[25]
{
  reg u64 i, t, inlen8;
  reg u8 c;

  inlen8 = inlen;
  inlen8 >>= 3;
  i = 0;
  while ( i < inlen8)
  {
    t = [in + 8*i];
    state[(int) i] ^= t;
    i = i + 1;
  }

  i <<= 3;
  while (i < inlen)
  {
    c = (u8)[in + i];
    state[u8 (int) i] ^= c;
    i = i + 1;
  }

  state[u8 (int) i] ^= trail_byte;

  i = r8;
  i -= 1;
  state[u8 (int) i] ^= 0x80;

  return state;
}


inline
fn __xtr_full_block(
  stack u64[25] state,
  reg u64 out,
  reg u64 outlen,
  reg u64 rate
) -> reg u64, reg u64
{
  reg u64 i t rate64;

  rate64 = rate;
  rate64 >>= 3;
  i = 0;
  while (i < rate64)
  {
    t = state[(int) i];
    [out + 8 * i] = t;
    i = i + 1;
  }

  out += rate;
  outlen -= rate;

  return out, outlen;
}


inline
fn __xtr_bytes(
  stack u64[25] state,
  reg u64 out,
  reg u64 outlen
)
{
  reg u64 i t outlen8;
  reg u8 c;

  outlen8 = outlen;
  outlen8 >>= 3;
  i = 0;
  while (i < outlen8 )
  {
    t = state[(int) i];
    [out + 8 * i] = t;
    i = i + 1;
  }
  i <<= 3;

  while (i < outlen)
  {
    c = state[u8 (int) i];
    (u8)[out + i] = c;
    i = i + 1;
  }
}


inline
fn __keccak1600_scalar(
  stack u64 s_out s_outlen,
  reg   u64 in inlen,
  stack u64 s_trail_byte,
  reg   u64 rate
)
{
  stack u64[25] state;
  stack u64 s_in, s_inlen, s_rate;
  reg u64 out, outlen, t;
  reg u8 trail_byte;

  state = __st0(state);

  while ( inlen >= rate )
  {
    state, in, inlen = __add_full_block(state, in, inlen, rate);

    s_in = in;
    s_inlen = inlen;
    s_rate = rate;

    state = _keccakf1600_(state);

    inlen = s_inlen;
    in = s_in;
    rate = s_rate;
  }

  t = s_trail_byte;
  trail_byte = (8u) t;
  state = __add_final_block(state, in, inlen, trail_byte, rate);

  outlen = s_outlen;

  while ( outlen > rate )
  {
    s_outlen = outlen;
    s_rate = rate;

    state = _keccakf1600_(state);

    out = s_out;
    outlen = s_outlen;
    rate = s_rate;

    out, outlen = __xtr_full_block(state, out, outlen, rate);
    s_outlen = outlen;
    s_out = out;
  }

  state = _keccakf1600_(state);
  out = s_out;
  outlen = s_outlen;

  __xtr_bytes(state, out, outlen);
}


#[returnaddress="stack"]
fn _shake256(reg u64 out outlen in inlen)
{
  stack u64 ds;
  stack u64 rate;

  ds = 0x1f;
  rate = SHAKE256_RATE;

  __keccak1600_scalar(out, outlen, in, inlen, ds, rate);
}


#[returnaddress="stack"]
fn _sha3_512(reg u64 out in inlen)
{
  reg u64 ds;
  reg u64 rate;
  reg u64 outlen;

  ds = 0x06;
  rate = SHA3_512_RATE;
  outlen = 64;

  __keccak1600_scalar(out, outlen, in, inlen, ds, rate);
}


#[returnaddress="stack"]
fn _sha3_256(reg u64 out in inlen)
{
  reg u64 ds;
  reg u64 rate;
  reg u64 outlen;

  ds = 0x06;
  rate = SHA3_256_RATE;
  outlen = 32;

  __keccak1600_scalar(out, outlen, in, inlen, ds, rate);
}


#[returnaddress="stack"]
fn _isha3_256(reg ptr u8[32] out, reg u64 in inlen) -> reg ptr u8[32]
{
  stack u64[25] state;
  stack ptr u8[32] s_out;
  stack u64 s_in s_ilen s_r8;
  reg u64 ilen r8 t64;
  reg u8 t8;
  inline int i;

  s_out = out;

  state = __st0(state);

  r8 = SHA3_256_RATE;
  ilen = inlen;

  while(ilen >= r8)
  {
    state, in, ilen = __add_full_block(state, in, ilen, r8);

    s_in = in;
    s_ilen = ilen;
    s_r8 = r8;

    state = _keccakf1600_(state);

    in = s_in;
    ilen = s_ilen;
    r8 = s_r8;
  }

  t8 = 0x06;
  state = __add_final_block(state, in, ilen, t8, r8);

  state = _keccakf1600_(state);

  out = s_out;

  for i=0 to 4
  {
    t64 = state[i];
    out[u64 i] = t64;
  }

  return out;
}

inline
fn __isha3_512(reg ptr u8[64] out, reg u64 in, inline int inlen) -> stack u8[64]
{
  stack u64[25] state;
  stack ptr u8[64] s_out;
  stack u64 s_in s_ilen s_r8;
  reg u64 ilen r8 t64;
  reg u8 t8;
  inline int i;

  s_out = out;

  state = __st0(state);

  r8 = SHA3_512_RATE;
  ilen = inlen;

  while(ilen >= r8)
  {
    state, in, ilen = __add_full_block(state, in, ilen, r8);

    s_in = in;
    s_ilen = ilen;
    s_r8 = r8;

    state = _keccakf1600_(state);

    in = s_in;
    ilen = s_ilen;
    r8 = s_r8;
  }

  t8 = 0x06;
  state = __add_final_block(state, in, ilen, t8, r8);

  state = _keccakf1600_(state);

  out = s_out;

  for i=0 to 8
  {
    t64 = state[i];
    out[u64 i] = t64;
  }

  return out;
}

fn _shake256_1120_32(reg u64 out, reg u64 in0 in1) {
  stack u64[25] state;
  stack u64 s_out;
  stack u64 s_in s_ilen s_r8;
  reg u64 ilen r8 t64 in;
  reg u8 t8;
  inline int i;

  s_out = out;
  state = __st0(state);

  for i = 0 to MLKEM_SYMBYTES/8 {
    t64 = (u64)[in0 + i*8];
    state[u64 i] ^= t64;
  }

  for i = MLKEM_SYMBYTES/8 to SHAKE256_RATE/8 {
    t64 = (u64)[in1 + (i-MLKEM_SYMBYTES/8)*8];
    state[u64 i] ^= t64;
  }

  s_in = in1;
      
  state = _keccakf1600_(state);

  r8 = SHAKE256_RATE;
  ilen = MLKEM_INDCPA_CIPHERTEXTBYTES - (SHAKE256_RATE - MLKEM_SYMBYTES);
  in = s_in;
  in += SHAKE256_RATE - MLKEM_SYMBYTES;

  while(ilen >= r8)
  {
    state, in, ilen = __add_full_block(state, in, ilen, r8);

    s_in = in;
    s_ilen = ilen;
    s_r8 = r8;

    state = _keccakf1600_(state);

    in = s_in;
    ilen = s_ilen;
    r8 = s_r8;
  }

  t8 = 0x1f;
  state = __add_final_block(state, in, ilen, t8, r8);

  state = _keccakf1600_(state);

  out = s_out;

  for i=0 to MLKEM_SYMBYTES/8
  {
    t64 = state[i];
    (u64)[out + 8*i] = t64;
  }

}

#[returnaddress="stack"]
fn _shake256_128_33(reg ptr u8[128] out, reg const ptr u8[33] in) -> stack u8[128]
{
  stack u64[25] state;
  reg u64 t64;
  reg u8 c;
  inline int i;

  stack ptr u8[128] sout;

  sout = out;

  state = __st0(state);

  for i = 0 to 4 {
    t64 = in[u64 i];
    state[u64 i] ^= t64;
  }

  c = in[32];
  state[u8 32] ^= c;
  state[u8 33] ^= 0x1f;
  state[u8 SHAKE256_RATE-1] ^= 0x80;

  state = _keccakf1600_(state);

  out = sout; 

  for i = 0 to 16 {
    t64 = state[u64 i];
    out[u64 i] = t64;
  }

  return out;
}

#[returnaddress="stack"]
fn _isha3_256_32(reg ptr u8[32] out, reg ptr u8[MLKEM_SYMBYTES] in) -> reg ptr u8[32]
{
  stack u64[25] state;
  stack ptr u8[32] s_out;
  reg u64 t64;
  inline int i;

  s_out = out;

  state = __st0(state);

  for i=0 to MLKEM_SYMBYTES/8
  {
    t64 = in[u64 i];
    state[u64 i] = t64;
  }

  state[u8 MLKEM_SYMBYTES] ^= 0x06;
  state[u8 SHA3_256_RATE - 1] = 0x80;

  state = _keccakf1600_(state);

  out = s_out;

  for i=0 to 4
  {
    t64 = state[i];
    out[u64 i] = t64;
  }

  return out;
}

#[returnaddress="stack"]
fn _sha3_512_64(reg ptr u8[64] out, reg const ptr u8[64] in) -> stack u8[64]
{
  stack u64[25] state;
  stack ptr u8[64] out_s;
  reg u64 t64;
  inline int i;

  state = __st0(state);

  for i = 0 to 8
  {
    t64 = in[u64 i];
    state[i] ^= t64;
  }

  state[u8 64] ^= 0x06;
  state[u8 SHA3_512_RATE - 1] ^= 0x80;

  out_s = out;

  state = _keccakf1600_(state);

  out = out_s;

  for i = 0 to 8
  {
    t64 = state[i];
    out[u64 i] = t64;
  }

  return out;
}

#[returnaddress="stack"]
fn _sha3_512_32(reg ptr u8[64] out, reg const ptr u8[32] in) -> stack u8[64]
{
  stack u64[25] state;
  stack ptr u8[64] out_s;
  reg u64 t64;
  inline int i;

  state = __st0(state);

  for i = 0 to 4
  {
    t64 = in[u64 i];
    state[i] ^= t64;
  }

  state[u8 32] ^= 0x06;
  state[u8 SHA3_512_RATE-1] ^= 0x80;

  out_s = out;
  
  state = _keccakf1600_(state);

  out = out_s;
  
  for i = 0 to 8 {
    t64 = state[i];
    out[u64 i] = t64;
  }

  return out;
}

fn _shake128_absorb34(reg ptr u64[25] state, reg const ptr u8[34] in) -> reg ptr u64[25]
{
  reg u64 t64;
  reg u16 t16;
  inline int i;

  state = __st0(state);

  for i = 0 to 4
  {
    t64 = in[u64 i];
    state[u64 i] ^= t64;
  }

  t16 = in.[u16 32];
  state[u16 16] ^= t16;

  state[u8 34] ^= 0x1f;

  state[u8 SHAKE128_RATE-1] ^= 0x80;

  return state;
}

#[returnaddress="stack"]
fn _shake128_squeezeblock(reg ptr u64[25] state, reg ptr u8[SHAKE128_RATE] out) -> reg ptr u64[25], reg ptr u8[SHAKE128_RATE]
{
  stack ptr u8[SHAKE128_RATE] out_s;
  reg u64 t;
  inline int i;

  out_s = out;
  state = _keccakf1600_(state);
  out = out_s;

  for i = 0 to SHAKE128_RATE/8
  {
    t = state[i];
    out[u64 i] = t;
  }
  return state, out;
}

u256 rho56 = 0x181F1E1D1C1B1A191017161514131211080F0E0D0C0B0A090007060504030201;
u256 rho8 = 0x1E1D1C1B1A19181F16151413121110170E0D0C0B0A09080F0605040302010007;

inline fn __rol_4u64_rho56(reg u256 a) -> reg u256
{
	reg u256 r;

	r = #VPSHUFB_256(a, rho56);

	return r; 
}


inline fn __rol_4u64_rho8(reg u256 a) -> reg u256
{
	reg u256 r;

	r = #VPSHUFB_256(a, rho8);

	return r; 
}


inline fn __rol_4u64(reg u256 a, inline int o) -> reg u256
{
	reg u256 r;
	reg u256 t256;

	r = #VPSLL_4u64(a, o);
	t256 = #VPSRL_4u64(a, 64 - o);

	r |= t256;

	return r; 
}


param int ba=0;
param int be=1;
param int bi=2;
param int bo=3;
param int bu=4;
param int ga=5;
param int ge=6;
param int gi=7;
param int go=8;
param int gu=9;
param int ka=10;
param int ke=11;
param int ki=12;
param int ko=13;
param int ku=14;
param int ma=15;
param int me=16;
param int mi=17;
param int mo=18;
param int mu=19;
param int sa=20;
param int se=21;
param int si=22;
param int so=23;
param int su=24;

u256[24] KeccakF1600RoundConstants = {
  0x0000000000000001000000000000000100000000000000010000000000000001,
    0x0000000000008082000000000000808200000000000080820000000000008082,
    0x800000000000808a800000000000808a800000000000808a800000000000808a,
    0x8000000080008000800000008000800080000000800080008000000080008000,
    0x000000000000808b000000000000808b000000000000808b000000000000808b,
    0x0000000080000001000000008000000100000000800000010000000080000001,
    0x8000000080008081800000008000808180000000800080818000000080008081,
    0x8000000000008009800000000000800980000000000080098000000000008009,
    0x000000000000008a000000000000008a000000000000008a000000000000008a,
    0x0000000000000088000000000000008800000000000000880000000000000088,
    0x0000000080008009000000008000800900000000800080090000000080008009,
    0x000000008000000a000000008000000a000000008000000a000000008000000a,
    0x000000008000808b000000008000808b000000008000808b000000008000808b,
    0x800000000000008b800000000000008b800000000000008b800000000000008b,
    0x8000000000008089800000000000808980000000000080898000000000008089,
    0x8000000000008003800000000000800380000000000080038000000000008003,
    0x8000000000008002800000000000800280000000000080028000000000008002,
    0x8000000000000080800000000000008080000000000000808000000000000080,
    0x000000000000800a000000000000800a000000000000800a000000000000800a,
    0x800000008000000a800000008000000a800000008000000a800000008000000a,
    0x8000000080008081800000008000808180000000800080818000000080008081,
    0x8000000000008080800000000000808080000000000080808000000000008080,
    0x0000000080000001000000008000000100000000800000010000000080000001,
    0x8000000080008008800000008000800880000000800080088000000080008008
    };

inline fn __prepare_theta(reg ptr u256[25] A_4x) -> reg u256, reg u256, reg u256, reg u256, reg u256
{ 
    reg u256 Ca, Ce, Ci, Co, Cu;

    // Ca = XOR256(Aba, XOR256(Aga, XOR256(Aka, XOR256(Ama, Asa))));
    Ca = A_4x[sa];
    Ca ^= A_4x[ma];
    Ca ^=  A_4x[ka];
    Ca ^=  A_4x[ga];
    Ca ^=  A_4x[ba];

    // Ce = XOR256(Abe, XOR256(Age, XOR256(Ake, XOR256(Ame, Ase))));
    Ce = A_4x[se];
    Ce ^= A_4x[me];
    Ce ^= A_4x[ke];
    Ce ^= A_4x[ge];
    Ce ^= A_4x[be];

    // Ci = XOR256(Abi, XOR256(Agi, XOR256(Aki, XOR256(Ami, Asi))));
    Ci = A_4x[si];
    Ci ^= A_4x[mi];
    Ci ^= A_4x[ki];
    Ci ^= A_4x[gi];
    Ci ^= A_4x[bi];

    // Co = XOR256(Abo, XOR256(Ago, XOR256(Ako, XOR256(Amo, Aso))));
    Co = A_4x[so];
    Co ^= A_4x[mo];
    Co ^= A_4x[ko];
    Co ^= A_4x[go];
    Co ^= A_4x[bo];

    // Cu = XOR256(Abu, XOR256(Agu, XOR256(Aku, XOR256(Amu, Asu))));
    Cu = A_4x[su];
    Cu ^= A_4x[mu];
    Cu ^= A_4x[ku];
    Cu ^= A_4x[gu];
    Cu ^= A_4x[bu];

    return Ca, Ce, Ci, Co, Cu;
}

inline fn __first(reg u256 Ca, reg u256 Ce, reg u256 Ci, reg u256 Co, reg u256 Cu) ->  reg u256, reg u256, reg u256, reg u256, reg u256
{
    reg u256 Da, De, Di, Do, Du;
    reg u256 Ca1, Ce1, Ci1, Co1, Cu1;

    Ce1 = __rol_4u64(Ce, 1);
    Da = Cu ^ Ce1;

    Ci1 = __rol_4u64(Ci, 1);
    De = Ca ^ Ci1;

    Co1 = __rol_4u64(Co, 1);
    Di = Ce ^ Co1;

    Cu1 = __rol_4u64(Cu, 1);
    Do = Ci ^ Cu1;

    Ca1 = __rol_4u64(Ca, 1);
    Du = Co ^ Ca1;

    return Da, De, Di, Do, Du;
}


inline fn __second_even(
reg ptr u256[25] A_4x, reg ptr u256[25] E_4x, inline int index,
reg u256 Ca, reg u256 Ce, reg u256 Ci, reg u256 Co, reg u256 Cu,
reg u256 Da, reg u256 De, reg u256 Di, reg u256 Do, reg u256 Du) 
-> reg ptr u256[25], reg ptr u256[25], reg u256, reg u256, reg u256, reg u256, reg u256
{
    reg u256 Bba, Bbe, Bbi, Bbo, Bbu;
    reg u256 t256;

    t256 = A_4x[ba];
    t256 ^= Da;
    A_4x[ba] = t256;
    Bba = t256;

    t256 = A_4x[ge];
    t256 ^= De;
    A_4x[ge] = t256;
    Bbe = __rol_4u64(t256, 44);

    t256 = A_4x[ki];
    t256 ^= Di;
    A_4x[ki] = t256;
    Bbi = __rol_4u64(t256, 43);

    // E##ba = XOR256(Bba, ANDnu256(Bbe, Bbi)); XOReq256(E##ba, CONST256_64(KeccakF1600RoundConstants[i]));
    t256 = #VPANDN_256(Bbe, Bbi);
    t256 ^= Bba;
    t256 ^= KeccakF1600RoundConstants[index];
    E_4x[ba] = t256;

    Ca = t256;

    t256 = A_4x[mo];
    t256 ^= Do;
    A_4x[mo] = t256;
    Bbo = __rol_4u64(t256, 21);

    //  E##be = XOR256(Bbe, ANDnu256(Bbi, Bbo));
    t256 = #VPANDN_256(Bbi, Bbo);
    t256 ^= Bbe;
    E_4x[be] = t256;

    Ce = t256;

    t256 = A_4x[su];
    t256 ^= Du;
    A_4x[su] = t256;
    Bbu = __rol_4u64(t256, 14);

    // E##bi = XOR256(Bbi, ANDnu256(Bbo, Bbu)); 
    t256 = #VPANDN_256(Bbo, Bbu);
    t256 ^= Bbi;
    E_4x[bi] = t256;

    Ci = t256;

    // E##bo = XOR256(Bbo, ANDnu256(Bbu, Bba));
    t256 = #VPANDN_256(Bbu, Bba);
    t256 ^= Bbo;
    E_4x[bo] = t256; 
    
    Co = t256; 
    
    // E##bu = XOR256(Bbu, ANDnu256(Bba, Bbe));
    t256 = #VPANDN_256(Bba, Bbe);
    t256 ^= Bbu; 
    E_4x[bu] = t256;

    Cu = t256;

    return A_4x, E_4x, Ca, Ce, Ci, Co, Cu;
}

inline fn __third_even(
reg ptr u256[25] A_4x, reg ptr u256[25] E_4x,
reg u256 Ca, reg u256 Ce, reg u256 Ci, reg u256 Co, reg u256 Cu,
reg u256 Da, reg u256 De, reg u256 Di, reg u256 Do, reg u256 Du) 
-> reg ptr u256[25], reg ptr u256[25], reg u256, reg u256, reg u256, reg u256, reg u256
{
    reg u256 Bga, Bge, Bgi, Bgo, Bgu;
    reg u256 t256;

    t256 = A_4x[bo];
    t256 ^= Do;
    A_4x[bo] = t256;
    Bga = __rol_4u64(t256, 28);

    t256 = A_4x[gu];
    t256 ^= Du;
    A_4x[gu] = t256;
    Bge = __rol_4u64(t256, 20);

    t256 = A_4x[ka];
    t256 ^= Da;
    A_4x[ka] = t256;
    Bgi = __rol_4u64(t256, 3);

    // E##ga = XOR256(Bga, ANDnu256(Bge, Bgi))
    t256 = #VPANDN_256(Bge, Bgi);
    t256 ^= Bga;
    E_4x[ga] = t256;

    Ca ^= t256;

    t256 = A_4x[me];
    t256 ^= De;
    A_4x[me] = t256;
    Bgo = __rol_4u64(t256, 45);

    // E##ge = XOR256(Bge, ANDnu256(Bgi, Bgo))
    t256 = #VPANDN_256(Bgi, Bgo);
    t256 ^= Bge;
    E_4x[ge] = t256;

    Ce ^= t256;

    t256 = A_4x[si];
    t256 ^= Di;
    A_4x[si] = t256;
    Bgu = __rol_4u64(t256, 61);

    //  E##gi = XOR256(Bgi, ANDnu256(Bgo, Bgu))
    t256 = #VPANDN_256(Bgo, Bgu);
    t256 ^= Bgi;
    E_4x[gi] = t256;
    
    Ci ^= t256;

    // E##go = XOR256(Bgo, ANDnu256(Bgu, Bga));
    t256 = #VPANDN_256(Bgu, Bga);
    t256 ^= Bgo;
    E_4x[go] = t256;
    
    Co ^= t256;

    // E##gu = XOR256(Bgu, ANDnu256(Bga, Bge));
    t256 = #VPANDN_256(Bga, Bge);
    t256 ^= Bgu;
    E_4x[gu] = t256;
    
    Cu ^= t256;

    return A_4x, E_4x, Ca, Ce, Ci, Co, Cu;
}

inline fn __fourth_even(
reg ptr u256[25] A_4x, reg ptr u256[25] E_4x,
reg u256 Ca, reg u256 Ce, reg u256 Ci, reg u256 Co, reg u256 Cu,
reg u256 Da, reg u256 De, reg u256 Di, reg u256 Do, reg u256 Du) 
-> reg ptr u256[25], reg ptr u256[25], reg u256, reg u256, reg u256, reg u256, reg u256
{
    reg u256 Bka, Bke, Bki, Bko, Bku;
    reg u256 t256;

    t256 = A_4x[be];
    t256 ^= De;
    A_4x[be] = t256;
    Bka = __rol_4u64(t256, 1);

    t256 = A_4x[gi];
    t256 ^= Di;
    A_4x[gi] = t256;
    Bke = __rol_4u64(t256, 6);

    t256 = A_4x[ko];
    t256 ^= Do;
    A_4x[ko] = t256;
    Bki = __rol_4u64(t256, 25);

    // E##ka = XOR256(Bka, ANDnu256(Bke, Bki));
    t256 = #VPANDN_256(Bke, Bki);
    t256 ^= Bka;
    E_4x[ka] = t256;

    Ca ^= t256;
    
    t256 = A_4x[mu];
    t256 ^= Du;
    A_4x[mu] = t256;
    Bko = __rol_4u64_rho8(t256);

    // E##ke = XOR256(Bke, ANDnu256(Bki, Bko));
    t256 = #VPANDN_256(Bki, Bko);
    t256 ^= Bke;
    E_4x[ke] = t256;

    Ce ^= t256;
    
    t256 = A_4x[sa];
    t256 ^= Da;
    A_4x[sa] = t256;
    Bku = __rol_4u64(t256, 18);

    // E##ki = XOR256(Bki, ANDnu256(Bko, Bku))
    t256 = #VPANDN_256(Bko, Bku);
    t256 ^= Bki;
    E_4x[ki] = t256;

    Ci ^= t256;

    //  E##ko = XOR256(Bko, ANDnu256(Bku, Bka));
    t256 = #VPANDN_256(Bku, Bka);
    t256 ^= Bko;
    E_4x[ko] = t256;

    Co ^= t256;

    //  E##ku = XOR256(Bku, ANDnu256(Bka, Bke));
    t256 = #VPANDN_256(Bka, Bke);
    t256 ^= Bku;
    E_4x[ku] = t256;

    Cu ^= t256;

    return A_4x, E_4x, Ca, Ce, Ci, Co, Cu;
}

inline fn __fifth_even(
reg ptr u256[25] A_4x, reg ptr u256[25] E_4x,
reg u256 Ca, reg u256 Ce, reg u256 Ci, reg u256 Co, reg u256 Cu,
reg u256 Da, reg u256 De, reg u256 Di, reg u256 Do, reg u256 Du) 
-> reg ptr u256[25], reg ptr u256[25], reg u256, reg u256, reg u256, reg u256, reg u256
{
    reg u256 Bma, Bme, Bmi, Bmo, Bmu;
    reg u256 t256;

    t256 = A_4x[bu];
    t256 ^= Du;
    A_4x[bu] = t256;
    Bma = __rol_4u64(t256, 27);

    t256 = A_4x[ga];
    t256 ^= Da;
    A_4x[ga] = t256;
    Bme = __rol_4u64(t256, 36);

    t256 = A_4x[ke];
    t256 ^= De;
    A_4x[ke] = t256;
    Bmi = __rol_4u64(t256, 10);

    // E##ma = XOR256(Bma, ANDnu256(Bme, Bmi));
    t256 = #VPANDN_256(Bme, Bmi);
    t256 ^= Bma;
    E_4x[ma] = t256;

    Ca ^= t256;

    t256 = A_4x[mi];
    t256 ^= Di;
    A_4x[mi] = t256;
    Bmo = __rol_4u64(t256, 15);

    // E##me = XOR256(Bme, ANDnu256(Bmi, Bmo));
    t256 = #VPANDN_256(Bmi, Bmo);
    t256 ^= Bme;
    E_4x[me] = t256;

    Ce ^= t256;

    t256 = A_4x[so];
    t256 ^= Do;
    A_4x[so] = t256;
    Bmu = __rol_4u64_rho56(t256);

    // E##mi = XOR256(Bmi, ANDnu256(Bmo, Bmu));
    t256 = #VPANDN_256(Bmo, Bmu);
    t256 ^= Bmi;
    E_4x[mi] = t256;

    Ci ^= t256;

    // E##mo = XOR256(Bmo, ANDnu256(Bmu, Bma));
    t256 = #VPANDN_256(Bmu, Bma);
    t256 ^= Bmo;
    E_4x[mo] = t256;

    Co ^= t256;

    // E##mu = XOR256(Bmu, ANDnu256(Bma, Bme));
    t256 = #VPANDN_256(Bma, Bme);
    t256 ^= Bmu;
    E_4x[mu] = t256;

    Cu ^= t256;

    return A_4x, E_4x, Ca, Ce, Ci, Co, Cu;
}

inline fn __sixth_even(
reg ptr u256[25] A_4x, reg ptr u256[25] E_4x,
reg u256 Ca, reg u256 Ce, reg u256 Ci, reg u256 Co, reg u256 Cu,
reg u256 Da, reg u256 De, reg u256 Di, reg u256 Do, reg u256 Du) 
-> reg ptr u256[25], reg ptr u256[25], reg u256, reg u256, reg u256, reg u256, reg u256
{
    reg u256 Bsa, Bse, Bsi, Bso, Bsu;
    reg u256 t256;

    t256 = A_4x[bi];
    t256 ^= Di;
    A_4x[bi] = t256;
    Bsa = __rol_4u64(t256, 62);

    t256 = A_4x[go];
    t256 ^= Do;
    A_4x[go] = t256;
    Bse = __rol_4u64(t256, 55);

    t256 = A_4x[ku];
    t256 ^= Du;
    A_4x[ku] = t256;
    Bsi = __rol_4u64(t256, 39);

    // E##sa = XOR256(Bsa, ANDnu256(Bse, Bsi));
    t256 = #VPANDN_256(Bse, Bsi);
    t256 ^= Bsa;
    E_4x[sa] = t256;

    Ca ^= t256;

    t256 = A_4x[ma];
    t256 ^= Da;
    A_4x[ma] = t256;
    Bso = __rol_4u64(t256, 41);

    // E##se = XOR256(Bse, ANDnu256(Bsi, Bso))
    t256 = #VPANDN_256(Bsi, Bso);
    t256 ^= Bse;
    E_4x[se] = t256;  

    Ce ^= t256;

    t256 = A_4x[se];
    t256 ^= De;
    A_4x[se] = t256;
    Bsu = __rol_4u64(t256, 2);

    // E##si = XOR256(Bsi, ANDnu256(Bso, Bsu)); 
    t256 = #VPANDN_256(Bso, Bsu);
    t256 ^= Bsi;
    E_4x[si] = t256;

    Ci ^= t256;

    // E##so = XOR256(Bso, ANDnu256(Bsu, Bsa));
    t256 = #VPANDN_256(Bsu, Bsa);
    t256 ^= Bso;
    E_4x[so] = t256;

    Co ^= t256;

    // E##su = XOR256(Bsu, ANDnu256(Bsa, Bse));
    t256 = #VPANDN_256(Bsa, Bse);
    t256 ^= Bsu;
    E_4x[su] = t256;

    Cu ^= t256;

    return A_4x, E_4x, Ca, Ce, Ci, Co, Cu;
}

inline fn __second_odd(
reg ptr u256[25] A_4x, reg ptr u256[25] E_4x, inline int index,
reg u256 Ca, reg u256 Ce, reg u256 Ci, reg u256 Co, reg u256 Cu,
reg u256 Da, reg u256 De, reg u256 Di, reg u256 Do, reg u256 Du) 
-> reg ptr u256[25], reg ptr u256[25], reg u256, reg u256, reg u256, reg u256, reg u256
{
    reg u256 Bba, Bbe, Bbi, Bbo, Bbu;
    reg u256 t256;

    t256 = A_4x[ba];
    t256 ^= Da;
    A_4x[ba] = t256;
    Bba = t256;

    t256 = A_4x[ge];
    t256 ^= De;
    A_4x[ge] = t256;
    Bbe = __rol_4u64(t256, 44);

    t256 = A_4x[ki];
    t256 ^= Di;
    A_4x[ki] = t256;
    Bbi = __rol_4u64(t256, 43);

    // E##ba = XOR256(Bba, ANDnu256(Bbe, Bbi)); XOReq256(E##ba, CONST256_64(KeccakF1600RoundConstants[i]));
    t256 = #VPANDN_256(Bbe, Bbi);
    t256 ^= Bba;
    t256 ^= KeccakF1600RoundConstants[index];
    E_4x[ba] = t256;

    Ca = t256;

    t256 = A_4x[mo];
    t256 ^= Do;
    A_4x[mo] = t256;
    Bbo = __rol_4u64(t256, 21);

    //  E##be = XOR256(Bbe, ANDnu256(Bbi, Bbo));
    t256 = #VPANDN_256(Bbi, Bbo);
    t256 ^= Bbe;
    E_4x[be] = t256;

    Ce = t256;

    t256 = A_4x[su];
    t256 ^= Du;
    A_4x[su] = t256;
    Bbu = __rol_4u64(t256, 14);

    // E##bi = XOR256(Bbi, ANDnu256(Bbo, Bbu)); 
    t256 = #VPANDN_256(Bbo, Bbu);
    t256 ^= Bbi;
    E_4x[bi] = t256;

    Ci = t256;

    // E##bo = XOR256(Bbo, ANDnu256(Bbu, Bba));
    t256 = #VPANDN_256(Bbu, Bba);
    t256 ^= Bbo;
    E_4x[bo] = t256; 
    
    Co = t256; 
    
    // E##bu = XOR256(Bbu, ANDnu256(Bba, Bbe));
    t256 = #VPANDN_256(Bba, Bbe);
    t256 ^= Bbu; 
    E_4x[bu] = t256;

    Cu = t256;

    return A_4x, E_4x, Ca, Ce, Ci, Co, Cu;
}

inline fn __third_odd(
reg ptr u256[25] A_4x, reg ptr u256[25] E_4x,
reg u256 Ca, reg u256 Ce, reg u256 Ci, reg u256 Co, reg u256 Cu,
reg u256 Da, reg u256 De, reg u256 Di, reg u256 Do, reg u256 Du) 
-> reg ptr u256[25], reg ptr u256[25], reg u256, reg u256, reg u256, reg u256, reg u256
{
    reg u256 Bga, Bge, Bgi, Bgo, Bgu;
    reg u256 t256;

    t256 = A_4x[bo];
    t256 ^= Do;
    A_4x[bo] = t256;
    Bga = __rol_4u64(t256, 28);

    t256 = A_4x[gu];
    t256 ^= Du;
    A_4x[gu] = t256;
    Bge = __rol_4u64(t256, 20);

    t256 = A_4x[ka];
    t256 ^= Da;
    A_4x[ka] = t256;
    Bgi = __rol_4u64(t256, 3);   

    // E##ga = XOR256(Bga, ANDnu256(Bge, Bgi))
    t256 = #VPANDN_256(Bge, Bgi);
    t256 ^= Bga;
    E_4x[ga] = t256;

    Ca ^= t256;

    t256 = A_4x[me];
    t256 ^= De;
    A_4x[me] = t256;
    Bgo = __rol_4u64(t256, 45);

    // E##ge = XOR256(Bge, ANDnu256(Bgi, Bgo))
    t256 = #VPANDN_256(Bgi, Bgo);
    t256 ^= Bge;
    E_4x[ge] = t256;

    Ce ^= t256;

    t256 = A_4x[si];
    t256 ^= Di;
    A_4x[si] = t256;
    Bgu = __rol_4u64(t256, 61);

    //  E##gi = XOR256(Bgi, ANDnu256(Bgo, Bgu))
    t256 = #VPANDN_256(Bgo, Bgu);
    t256 ^= Bgi;
    E_4x[gi] = t256;
    
    Ci ^= t256;

    // E##go = XOR256(Bgo, ANDnu256(Bgu, Bga));
    t256 = #VPANDN_256(Bgu, Bga);
    t256 ^= Bgo;
    E_4x[go] = t256;
    
    Co ^= t256;

    // E##gu = XOR256(Bgu, ANDnu256(Bga, Bge));
    t256 = #VPANDN_256(Bga, Bge);
    t256 ^= Bgu;
    E_4x[gu] = t256;
    
    Cu ^= t256;

    return A_4x, E_4x, Ca, Ce, Ci, Co, Cu;
}

inline fn __fourth_odd(
reg ptr u256[25] A_4x, reg ptr u256[25] E_4x,
reg u256 Ca, reg u256 Ce, reg u256 Ci, reg u256 Co, reg u256 Cu,
reg u256 Da, reg u256 De, reg u256 Di, reg u256 Do, reg u256 Du) 
-> reg ptr u256[25], reg ptr u256[25], reg u256, reg u256, reg u256, reg u256, reg u256
{
    reg u256 Bka, Bke, Bki, Bko, Bku;
    reg u256 t256;

    t256 = A_4x[be];
    t256 ^= De;
    A_4x[be] = t256;
    Bka = __rol_4u64(t256, 1);

    t256 = A_4x[gi];
    t256 ^= Di;
    A_4x[gi] = t256;
    Bke = __rol_4u64(t256, 6);

    t256 = A_4x[ko];
    t256 ^= Do;
    A_4x[ko] = t256;
    Bki = __rol_4u64(t256, 25);

    // E##ka = XOR256(Bka, ANDnu256(Bke, Bki));
    t256 = #VPANDN_256(Bke, Bki);
    t256 ^= Bka;
    E_4x[ka] = t256;

    Ca ^= t256;
    
    t256 = A_4x[mu];
    t256 ^= Du;
    A_4x[mu] = t256;
    Bko = __rol_4u64_rho8(t256);

    // E##ke = XOR256(Bke, ANDnu256(Bki, Bko));
    t256 = #VPANDN_256(Bki, Bko);
    t256 ^= Bke;
    E_4x[ke] = t256;

    Ce ^= t256;
    
    t256 = A_4x[sa];
    t256 ^= Da;
    A_4x[sa] = t256;
    Bku = __rol_4u64(t256, 18);

    // E##ki = XOR256(Bki, ANDnu256(Bko, Bku))
    t256 = #VPANDN_256(Bko, Bku);
    t256 ^= Bki;
    E_4x[ki] = t256;

    Ci ^= t256;

    //  E##ko = XOR256(Bko, ANDnu256(Bku, Bka));
    t256 = #VPANDN_256(Bku, Bka);
    t256 ^= Bko;
    E_4x[ko] = t256;

    Co ^= t256;

    //  E##ku = XOR256(Bku, ANDnu256(Bka, Bke));
    t256 = #VPANDN_256(Bka, Bke);
    t256 ^= Bku;
    E_4x[ku] = t256;

    Cu ^= t256;

    return A_4x, E_4x, Ca, Ce, Ci, Co, Cu;
}

inline fn __fifth_odd(
reg ptr u256[25] A_4x, reg ptr u256[25] E_4x,
reg u256 Ca, reg u256 Ce, reg u256 Ci, reg u256 Co, reg u256 Cu,
reg u256 Da, reg u256 De, reg u256 Di, reg u256 Do, reg u256 Du) 
-> reg ptr u256[25], reg ptr u256[25], reg u256, reg u256, reg u256, reg u256, reg u256
{
    reg u256 Bma, Bme, Bmi, Bmo, Bmu;
    reg u256 t256;

    t256 = A_4x[bu];
    t256 ^= Du;
    A_4x[bu] = t256;
    Bma = __rol_4u64(t256, 27);

    t256 = A_4x[ga];
    t256 ^= Da;
    A_4x[ga] = t256;
    Bme = __rol_4u64(t256, 36);

    t256 = A_4x[ke];
    t256 ^= De;
    A_4x[ke] = t256;
    Bmi = __rol_4u64(t256, 10);

    // E##ma = XOR256(Bma, ANDnu256(Bme, Bmi));
    t256 = #VPANDN_256(Bme, Bmi);
    t256 ^= Bma;
    E_4x[ma] = t256;

    Ca ^= t256;

    t256 = A_4x[mi];
    t256 ^= Di;
    A_4x[mi] = t256;
    Bmo = __rol_4u64(t256, 15);

    // E##me = XOR256(Bme, ANDnu256(Bmi, Bmo));
    t256 = #VPANDN_256(Bmi, Bmo);
    t256 ^= Bme;
    E_4x[me] = t256;

    Ce ^= t256;

    t256 = A_4x[so];
    t256 ^= Do;
    A_4x[so] = t256;
    Bmu = __rol_4u64_rho56(t256);

    // E##mi = XOR256(Bmi, ANDnu256(Bmo, Bmu));
    t256 = #VPANDN_256(Bmo, Bmu);
    t256 ^= Bmi;
    E_4x[mi] = t256;

    Ci ^= t256;

    // E##mo = XOR256(Bmo, ANDnu256(Bmu, Bma));
    t256 = #VPANDN_256(Bmu, Bma);
    t256 ^= Bmo;
    E_4x[mo] = t256;

    Co ^= t256;

    // E##mu = XOR256(Bmu, ANDnu256(Bma, Bme));
    t256 = #VPANDN_256(Bma, Bme);
    t256 ^= Bmu;
    E_4x[mu] = t256;

    Cu ^= t256;

    return A_4x, E_4x, Ca, Ce, Ci, Co, Cu;
}

inline fn __sixth_odd(
reg ptr u256[25] A_4x, reg ptr u256[25] E_4x,
reg u256 Ca, reg u256 Ce, reg u256 Ci, reg u256 Co, reg u256 Cu,
reg u256 Da, reg u256 De, reg u256 Di, reg u256 Do, reg u256 Du) 
-> reg ptr u256[25], reg ptr u256[25], reg u256, reg u256, reg u256, reg u256, reg u256
{
    reg u256 Bsa, Bse, Bsi, Bso, Bsu;
    reg u256 t256;

    t256 = A_4x[bi];
    t256 ^= Di;
    A_4x[bi] = t256;
    Bsa = __rol_4u64(t256, 62);

    t256 = A_4x[go];
    t256 ^= Do;
    A_4x[go] = t256;
    Bse = __rol_4u64(t256, 55);

    t256 = A_4x[ku];
    t256 ^= Du;
    A_4x[ku] = t256;
    Bsi = __rol_4u64(t256, 39);

    // E##sa = XOR256(Bsa, ANDnu256(Bse, Bsi));
    t256 = #VPANDN_256(Bse, Bsi);
    t256 ^= Bsa;
    E_4x[sa] = t256;

    Ca ^= t256;

    t256 = A_4x[ma];
    t256 ^= Da;
    A_4x[ma] = t256;
    Bso = __rol_4u64(t256, 41);

    // E##se = XOR256(Bse, ANDnu256(Bsi, Bso))
    t256 = #VPANDN_256(Bsi, Bso);
    t256 ^= Bse;
    E_4x[se] = t256;  

    Ce ^= t256;

    t256 = A_4x[se];
    t256 ^= De;
    A_4x[se] = t256;
    Bsu = __rol_4u64(t256, 2);

    // E##si = XOR256(Bsi, ANDnu256(Bso, Bsu)); 
    t256 = #VPANDN_256(Bso, Bsu);
    t256 ^= Bsi;
    E_4x[si] = t256;

    Ci ^= t256;

    // E##so = XOR256(Bso, ANDnu256(Bsu, Bsa));
    t256 = #VPANDN_256(Bsu, Bsa);
    t256 ^= Bso;
    E_4x[so] = t256;

    Co ^= t256;

    // E##su = XOR256(Bsu, ANDnu256(Bsa, Bse));
    t256 = #VPANDN_256(Bsa, Bse);
    t256 ^= Bsu;
    E_4x[su] = t256;

    Cu ^= t256;

    return A_4x, E_4x, Ca, Ce, Ci, Co, Cu;
}

inline fn __second_last(
reg ptr u256[25] A_4x, reg ptr u256[25] E_4x, inline int index,
reg u256 Da, reg u256 De, reg u256 Di, reg u256 Do, reg u256 Du) 
-> reg ptr u256[25], reg ptr u256[25]
{
    reg u256 Bba, Bbe, Bbi, Bbo, Bbu;
    reg u256 t256;

    t256 = A_4x[ba];
    t256 ^= Da;
    A_4x[ba] = t256;
    Bba = t256;

    t256 = A_4x[ge];
    t256 ^= De;
    A_4x[ge] = t256;
    Bbe = __rol_4u64(t256, 44);

    t256 = A_4x[ki];
    t256 ^= Di;
    A_4x[ki] = t256;
    Bbi = __rol_4u64(t256, 43);

    // E##ba = XOR256(Bba, ANDnu256(Bbe, Bbi)); XOReq256(E##ba, CONST256_64(KeccakF1600RoundConstants[i]));
    t256 = #VPANDN_256(Bbe, Bbi);
    t256 ^= Bba;
    t256 ^= KeccakF1600RoundConstants[index];
    E_4x[ba] = t256;

    t256 = A_4x[mo];
    t256 ^= Do;
    A_4x[mo] = t256;
    Bbo = __rol_4u64(t256, 21);

    //  E##be = XOR256(Bbe, ANDnu256(Bbi, Bbo));
    t256 = #VPANDN_256(Bbi, Bbo);
    t256 ^= Bbe;
    E_4x[be] = t256;

    t256 = A_4x[su];
    t256 ^= Du;
    A_4x[su] = t256;
    Bbu = __rol_4u64(t256, 14);

    // E##bi = XOR256(Bbi, ANDnu256(Bbo, Bbu)); 
    t256 = #VPANDN_256(Bbo, Bbu);
    t256 ^= Bbi;
    E_4x[bi] = t256;

    // E##bo = XOR256(Bbo, ANDnu256(Bbu, Bba));
    t256 = #VPANDN_256(Bbu, Bba);
    t256 ^= Bbo;
    E_4x[bo] = t256;
    
    // E##bu = XOR256(Bbu, ANDnu256(Bba, Bbe));
    t256 = #VPANDN_256(Bba, Bbe);
    t256 ^= Bbu; 
    E_4x[bu] = t256;

    return A_4x, E_4x;
}

inline fn __third_last(
reg ptr u256[25] A_4x, reg ptr u256[25] E_4x,
reg u256 Da, reg u256 De, reg u256 Di, reg u256 Do, reg u256 Du) 
-> reg ptr u256[25], reg ptr u256[25]
{
    reg u256 Bga, Bge, Bgi, Bgo, Bgu;
    reg u256 t256;

    t256 = A_4x[bo];
    t256 ^= Do;
    A_4x[bo] = t256;
    Bga = __rol_4u64(t256, 28);

    t256 = A_4x[gu];
    t256 ^= Du;
    A_4x[gu] = t256;
    Bge = __rol_4u64(t256, 20);

    t256 = A_4x[ka];
    t256 ^= Da;
    A_4x[ka] = t256;
    Bgi = __rol_4u64(t256, 3);   

    // E##ga = XOR256(Bga, ANDnu256(Bge, Bgi))
    t256 = #VPANDN_256(Bge, Bgi);
    t256 ^= Bga;
    E_4x[ga] = t256;

    t256 = A_4x[me];
    t256 ^= De;
    A_4x[me] = t256;
    Bgo = __rol_4u64(t256, 45);

    // E##ge = XOR256(Bge, ANDnu256(Bgi, Bgo))
    t256 = #VPANDN_256(Bgi, Bgo);
    t256 ^= Bge;
    E_4x[ge] = t256;

    t256 = A_4x[si];
    t256 ^= Di;
    A_4x[si] = t256;
    Bgu = __rol_4u64(t256, 61);

    //  E##gi = XOR256(Bgi, ANDnu256(Bgo, Bgu))
    t256 = #VPANDN_256(Bgo, Bgu);
    t256 ^= Bgi;
    E_4x[gi] = t256;

    // E##go = XOR256(Bgo, ANDnu256(Bgu, Bga));
    t256 = #VPANDN_256(Bgu, Bga);
    t256 ^= Bgo;
    E_4x[go] = t256;

    // E##gu = XOR256(Bgu, ANDnu256(Bga, Bge));
    t256 = #VPANDN_256(Bga, Bge);
    t256 ^= Bgu;
    E_4x[gu] = t256;

    return A_4x, E_4x;
}

inline fn __fourth_last(
reg ptr u256[25] A_4x, reg ptr u256[25] E_4x,
reg u256 Da, reg u256 De, reg u256 Di, reg u256 Do, reg u256 Du) 
-> reg ptr u256[25], reg ptr u256[25]
{
    reg u256 Bka, Bke, Bki, Bko, Bku;
    reg u256 t256;

    t256 = A_4x[be];
    t256 ^= De;
    A_4x[be] = t256;
    Bka = __rol_4u64(t256, 1);

    t256 = A_4x[gi];
    t256 ^= Di;
    A_4x[gi] = t256;
    Bke = __rol_4u64(t256, 6);

    t256 = A_4x[ko];
    t256 ^= Do;
    A_4x[ko] = t256;
    Bki = __rol_4u64(t256, 25);

    // E##ka = XOR256(Bka, ANDnu256(Bke, Bki));
    t256 = #VPANDN_256(Bke, Bki);
    t256 ^= Bka;
    E_4x[ka] = t256;
    
    t256 = A_4x[mu];
    t256 ^= Du;
    A_4x[mu] = t256;
    Bko = __rol_4u64_rho8(t256);

    // E##ke = XOR256(Bke, ANDnu256(Bki, Bko));
    t256 = #VPANDN_256(Bki, Bko);
    t256 ^= Bke;
    E_4x[ke] = t256;
    
    t256 = A_4x[sa];
    t256 ^= Da;
    A_4x[sa] = t256;
    Bku = __rol_4u64(t256, 18);

    // E##ki = XOR256(Bki, ANDnu256(Bko, Bku))
    t256 = #VPANDN_256(Bko, Bku);
    t256 ^= Bki;
    E_4x[ki] = t256;

    //  E##ko = XOR256(Bko, ANDnu256(Bku, Bka));
    t256 = #VPANDN_256(Bku, Bka);
    t256 ^= Bko;
    E_4x[ko] = t256;

    //  E##ku = XOR256(Bku, ANDnu256(Bka, Bke));
    t256 = #VPANDN_256(Bka, Bke);
    t256 ^= Bku;
    E_4x[ku] = t256;

    return A_4x, E_4x;
}

inline fn __fifth_last(
reg ptr u256[25] A_4x, reg ptr u256[25] E_4x,
reg u256 Da, reg u256 De, reg u256 Di, reg u256 Do, reg u256 Du) 
-> reg ptr u256[25], reg ptr u256[25]
{
    reg u256 Bma, Bme, Bmi, Bmo, Bmu;
    reg u256 t256;

    t256 = A_4x[bu];
    t256 ^= Du;
    A_4x[bu] = t256;
    Bma = __rol_4u64(t256, 27);

    t256 = A_4x[ga];
    t256 ^= Da;
    A_4x[ga] = t256;
    Bme = __rol_4u64(t256, 36);

    t256 = A_4x[ke];
    t256 ^= De;
    A_4x[ke] = t256;
    Bmi = __rol_4u64(t256, 10);

    // E##ma = XOR256(Bma, ANDnu256(Bme, Bmi));
    t256 = #VPANDN_256(Bme, Bmi);
    t256 ^= Bma;
    E_4x[ma] = t256;

    t256 = A_4x[mi];
    t256 ^= Di;
    A_4x[mi] = t256;
    Bmo = __rol_4u64(t256, 15);

    // E##me = XOR256(Bme, ANDnu256(Bmi, Bmo));
    t256 = #VPANDN_256(Bmi, Bmo);
    t256 ^= Bme;
    E_4x[me] = t256;

    t256 = A_4x[so];
    t256 ^= Do;
    A_4x[so] = t256;
    Bmu = __rol_4u64_rho56(t256);

    // E##mi = XOR256(Bmi, ANDnu256(Bmo, Bmu));
    t256 = #VPANDN_256(Bmo, Bmu);
    t256 ^= Bmi;
    E_4x[mi] = t256;

    // E##mo = XOR256(Bmo, ANDnu256(Bmu, Bma));
    t256 = #VPANDN_256(Bmu, Bma);
    t256 ^= Bmo;
    E_4x[mo] = t256;

    // E##mu = XOR256(Bmu, ANDnu256(Bma, Bme));
    t256 = #VPANDN_256(Bma, Bme);
    t256 ^= Bmu;
    E_4x[mu] = t256;

    return A_4x, E_4x;
}

inline fn __sixth_last(
reg ptr u256[25] A_4x, reg ptr u256[25] E_4x,
reg u256 Da, reg u256 De, reg u256 Di, reg u256 Do, reg u256 Du) 
-> reg ptr u256[25], reg ptr u256[25]
{
    reg u256 Bsa, Bse, Bsi, Bso, Bsu;
    reg u256 t256;

    t256 = A_4x[bi];
    t256 ^= Di;
    A_4x[bi] = t256;
    Bsa = __rol_4u64(t256, 62);

    t256 = A_4x[go];
    t256 ^= Do;
    A_4x[go] = t256;
    Bse = __rol_4u64(t256, 55);

    t256 = A_4x[ku];
    t256 ^= Du;
    A_4x[ku] = t256;
    Bsi = __rol_4u64(t256, 39);

    // E##sa = XOR256(Bsa, ANDnu256(Bse, Bsi));
    t256 = #VPANDN_256(Bse, Bsi);
    t256 ^= Bsa;
    E_4x[sa] = t256;

    t256 = A_4x[ma];
    t256 ^= Da;
    A_4x[ma] = t256;
    Bso = __rol_4u64(t256, 41);

    // E##se = XOR256(Bse, ANDnu256(Bsi, Bso))
    t256 = #VPANDN_256(Bsi, Bso);
    t256 ^= Bse;
    E_4x[se] = t256;

    t256 = A_4x[se];
    t256 ^= De;
    A_4x[se] = t256;
    Bsu = __rol_4u64(t256, 2);

    // E##si = XOR256(Bsi, ANDnu256(Bso, Bsu)); 
    t256 = #VPANDN_256(Bso, Bsu);
    t256 ^= Bsi;
    E_4x[si] = t256;

    // E##so = XOR256(Bso, ANDnu256(Bsu, Bsa));
    t256 = #VPANDN_256(Bsu, Bsa);
    t256 ^= Bso;
    E_4x[so] = t256;

    // E##su = XOR256(Bsu, ANDnu256(Bsa, Bse));
    t256 = #VPANDN_256(Bsa, Bse);
    t256 ^= Bsu;
    E_4x[su] = t256;

    return A_4x, E_4x;
}

inline fn __theta_rho_pi_chi_iota_prepare_theta_even(
reg ptr u256[25] A_4x, reg ptr u256[25] E_4x, inline int index,
reg u256 Ca, reg u256 Ce, reg u256 Ci, reg u256 Co, reg u256 Cu) 
-> reg ptr u256[25], reg ptr u256[25], reg u256, reg u256, reg u256, reg u256, reg u256
{   
    reg u256 Da, De, Di, Do, Du;

    Da, De, Di, Do, Du = __first(Ca, Ce, Ci, Co, Cu);

    A_4x, E_4x, Ca, Ce, Ci, Co, Cu = __second_even(A_4x, E_4x, index, Ca, Ce, Ci, Co, Cu, Da, De, Di, Do, Du);

    A_4x, E_4x, Ca, Ce, Ci, Co, Cu = __third_even(A_4x, E_4x, Ca, Ce, Ci, Co, Cu, Da, De, Di, Do, Du);
  
    A_4x, E_4x, Ca, Ce, Ci, Co, Cu = __fourth_even(A_4x, E_4x, Ca, Ce, Ci, Co, Cu, Da, De, Di, Do, Du);

    A_4x, E_4x, Ca, Ce, Ci, Co, Cu = __fifth_even(A_4x, E_4x, Ca, Ce, Ci, Co, Cu, Da, De, Di, Do, Du);

    A_4x, E_4x, Ca, Ce, Ci, Co, Cu = __sixth_even(A_4x, E_4x, Ca, Ce, Ci, Co, Cu, Da, De, Di, Do, Du);

    return A_4x, E_4x, Ca, Ce, Ci, Co, Cu;
}

inline fn __theta_rho_pi_chi_iota_prepare_theta_odd(
reg ptr u256[25] A_4x, reg ptr u256[25] E_4x, inline int index,
reg u256 Ca, reg u256 Ce, reg u256 Ci, reg u256 Co, reg u256 Cu) 
-> reg ptr u256[25], reg ptr u256[25], reg u256, reg u256, reg u256, reg u256, reg u256
{   
    reg u256 Da, De, Di, Do, Du;

    Da, De, Di, Do, Du = __first(Ca, Ce, Ci, Co, Cu);

    A_4x, E_4x, Ca, Ce, Ci, Co, Cu = __second_odd(A_4x, E_4x, index, Ca, Ce, Ci, Co, Cu, Da, De, Di, Do, Du);

    A_4x, E_4x, Ca, Ce, Ci, Co, Cu = __third_odd(A_4x, E_4x, Ca, Ce, Ci, Co, Cu, Da, De, Di, Do, Du);
  
    A_4x, E_4x, Ca, Ce, Ci, Co, Cu = __fourth_odd(A_4x, E_4x, Ca, Ce, Ci, Co, Cu, Da, De, Di, Do, Du);

    A_4x, E_4x, Ca, Ce, Ci, Co, Cu = __fifth_odd(A_4x, E_4x, Ca, Ce, Ci, Co, Cu, Da, De, Di, Do, Du);

    A_4x, E_4x, Ca, Ce, Ci, Co, Cu = __sixth_odd(A_4x, E_4x, Ca, Ce, Ci, Co, Cu, Da, De, Di, Do, Du);

    return A_4x, E_4x, Ca, Ce, Ci, Co, Cu;
}

inline fn __theta_rho_pi_chi_iota(
reg ptr u256[25] A_4x, reg ptr u256[25] E_4x, inline int index,
reg u256 Ca, reg u256 Ce, reg u256 Ci, reg u256 Co, reg u256 Cu) 
-> reg ptr u256[25], reg ptr u256[25]
{
    reg u256 Da, De, Di, Do, Du;

    Da, De, Di, Do, Du = __first(Ca, Ce, Ci, Co, Cu);

    A_4x, E_4x = __second_last(A_4x, E_4x, index, Da, De, Di, Do, Du);

    A_4x, E_4x = __third_last(A_4x, E_4x, Da, De, Di, Do, Du);

    A_4x, E_4x = __fourth_last(A_4x, E_4x, Da, De, Di, Do, Du);

    A_4x, E_4x  = __fifth_last(A_4x, E_4x, Da, De, Di, Do, Du);
 
    A_4x, E_4x  = __sixth_last(A_4x, E_4x, Da, De, Di, Do, Du);

    return A_4x, E_4x;
}

fn _KeccakF1600_StatePermute4x(reg ptr u256[25] A_4x) -> reg ptr u256[25]
{
    reg u256 Ca, Ce, Ci, Co, Cu;

    stack u256[25] E_4x;

    /** Rounds24 **/
    Ca, Ce, Ci, Co, Cu = __prepare_theta(A_4x);
    A_4x, E_4x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_even(A_4x, E_4x, 0, Ca, Ce, Ci, Co, Cu);
    E_4x, A_4x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_odd(E_4x, A_4x, 1, Ca, Ce, Ci, Co, Cu);
    A_4x, E_4x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_even(A_4x, E_4x, 2, Ca, Ce, Ci, Co, Cu); 
    E_4x, A_4x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_odd(E_4x, A_4x, 3, Ca, Ce, Ci, Co, Cu);
    A_4x, E_4x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_even(A_4x, E_4x, 4, Ca, Ce, Ci, Co, Cu);
    E_4x, A_4x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_odd(E_4x, A_4x, 5, Ca, Ce, Ci, Co, Cu);
    A_4x, E_4x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_even(A_4x, E_4x, 6, Ca, Ce, Ci, Co, Cu);
    E_4x, A_4x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_odd(E_4x, A_4x, 7, Ca, Ce, Ci, Co, Cu);
    A_4x, E_4x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_even(A_4x, E_4x, 8, Ca, Ce, Ci, Co, Cu);
    E_4x, A_4x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_odd(E_4x, A_4x, 9, Ca, Ce, Ci, Co, Cu);
    A_4x, E_4x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_even(A_4x, E_4x, 10, Ca, Ce, Ci, Co, Cu);
    E_4x, A_4x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_odd(E_4x, A_4x, 11, Ca, Ce, Ci, Co, Cu);
    A_4x, E_4x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_even(A_4x, E_4x, 12, Ca, Ce, Ci, Co, Cu);
    E_4x, A_4x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_odd(E_4x, A_4x, 13, Ca, Ce, Ci, Co, Cu);
    A_4x, E_4x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_even(A_4x, E_4x, 14, Ca, Ce, Ci, Co, Cu);
    E_4x, A_4x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_odd(E_4x, A_4x, 15, Ca, Ce, Ci, Co, Cu);
    A_4x, E_4x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_even(A_4x, E_4x, 16, Ca, Ce, Ci, Co, Cu);
    E_4x, A_4x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_odd(E_4x, A_4x, 17, Ca, Ce, Ci, Co, Cu);
    A_4x, E_4x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_even(A_4x, E_4x, 18, Ca, Ce, Ci, Co, Cu);
    E_4x, A_4x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_odd(E_4x, A_4x, 19, Ca, Ce, Ci, Co, Cu);
    A_4x, E_4x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_even(A_4x, E_4x, 20, Ca, Ce, Ci, Co, Cu);
    E_4x, A_4x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_odd(E_4x, A_4x, 21, Ca, Ce, Ci, Co, Cu);
    A_4x, E_4x, Ca, Ce, Ci, Co, Cu = __theta_rho_pi_chi_iota_prepare_theta_even(A_4x, E_4x, 22, Ca, Ce, Ci, Co, Cu);
    E_4x, A_4x = __theta_rho_pi_chi_iota(E_4x, A_4x, 23, Ca, Ce, Ci, Co, Cu);


    return A_4x;
}


fn _shake128_absorb4x_34(reg ptr u256[25] s, reg ptr u8[34] m0 m1 m2 m3) -> reg ptr u256[25]
{
	inline int i;
  reg u256 t0 t1;
  reg u16 t16;
	reg u64 t64;

  for i = 0 to 25
  {
    t0 = #set0_256();
    s[i] = t0;
  }

	for i = 0 to 4
  {
    t64 = m0[u64 i];
    s[u64 4 * i] ^= t64;
    t64 = m1[u64 i];
    s[u64 4 * i + 1] ^= t64;
    t64 = m2[u64 i];
    s[u64 4 * i + 2] ^= t64;
    t64 = m3[u64 i];
    s[u64 4 * i + 3] ^= t64;
  }

  t16 = m0.[u16 32];
  s[u16 64] ^= t16;
  s[u8 130] ^= 0x1F;

  t16 = m1.[u16 32];
  s[u16 68] ^= t16;
  s[u8 138] ^= 0x1F;

  t16 = m2.[u16 32];
  s[u16 72] ^= t16;
  s[u8 146] ^= 0x1F;

  t16 = m3.[u16 32];
  s[u16 76] ^= t16;
  s[u8 154] ^= 0x1F;

  t0 = shake_sep[u256 0];
  t1 = s[SHAKE128_RATE / 8 - 1];
  t0 = t0 ^ t1;
  s[SHAKE128_RATE / 8 - 1] = t0;

	return s;
}


inline
fn __shake128_squeezeblock4x(reg ptr u256[25] state, reg ptr u8[SHAKE128_RATE] h0 h1 h2 h3) -> reg ptr u256[25], reg ptr u8[SHAKE128_RATE], reg ptr u8[SHAKE128_RATE], reg ptr u8[SHAKE128_RATE], reg ptr u8[SHAKE128_RATE]
{
  reg u256 t256;
  reg u128 t128;
  inline int i;

  state = _KeccakF1600_StatePermute4x(state);

	for i = 0 to (SHAKE128_RATE / 8) {
    t256 = state[i];
    t128 = (128u)t256;
		h0[u64 i] = #VMOVLPD(t128);
		h1[u64 i] = #VMOVHPD(t128);
    t128 = #VEXTRACTI128(t256, 1);
		h2[u64 i] = #VMOVLPD(t128);
		h3[u64 i] = #VMOVHPD(t128);
	}

  return state, h0, h1, h2, h3;
}


fn _shake256_absorb4x_33(reg ptr u256[25] s, reg ptr u8[33] m0 m1 m2 m3) -> reg ptr u256[25]
{
	inline int i;
  reg u256 t0 t1;
	reg u64 t64;
  reg u8 t8;

  for i = 0 to 25
  {
    t0 = #set0_256();
    s[i] = t0;
  }

	for i = 0 to 4
  {
    t64 = m0[u64 i];
    s[u64 4 * i] ^= t64;
    t64 = m1[u64 i];
    s[u64 4 * i + 1] ^= t64;
    t64 = m2[u64 i];
    s[u64 4 * i + 2] ^= t64;
    t64 = m3[u64 i];
    s[u64 4 * i + 3] ^= t64;
  }

  t8 = m0[32];
  s[u8 128] ^= t8;
  s[u8 129] ^= 0x1F;

  t8 = m1[32];
  s[u8 136] ^= t8;
  s[u8 137] ^= 0x1F;

  t8 = m2[32];
  s[u8 144] ^= t8;
  s[u8 145] ^= 0x1F;

  t8 = m3[32];
  s[u8 152] ^= t8;
  s[u8 153] ^= 0x1F;

  t0 = shake_sep[u256 0];
  t1 = s[SHAKE256_RATE / 8 - 1];
  t0 = t0 ^ t1;
  s[SHAKE256_RATE / 8 - 1] = t0;

	return s;
}


inline
fn __shake256_squeezeblock4x(reg ptr u256[25] state, reg ptr u8[SHAKE256_RATE] h0 h1 h2 h3) -> reg ptr u256[25], reg ptr u8[SHAKE256_RATE], reg ptr u8[SHAKE256_RATE], reg ptr u8[SHAKE256_RATE], reg ptr u8[SHAKE256_RATE]
{
  reg u256 t256;
  reg u128 t128;
  inline int i;

  state = _KeccakF1600_StatePermute4x(state);

	for i = 0 to (SHAKE256_RATE / 8) {
    t256 = state[i];
    t128 = (128u)t256;
		h0[u64 i] = #VMOVLPD(t128);
		h1[u64 i] = #VMOVHPD(t128);
    t128 = #VEXTRACTI128(t256, 1);
		h2[u64 i] = #VMOVLPD(t128);
		h3[u64 i] = #VMOVHPD(t128);
	}

  return state, h0, h1, h2, h3;
}

fn _poly_add2(reg ptr u16[MLKEM_N] rp bp) -> stack u16[MLKEM_N]
{
  inline int i;
  reg u256 a;
  reg u256 b;
  reg u256 r;

  for i = 0 to 16 {
    a = rp.[u256 32*i];
    b = bp.[u256 32*i];
    r = #VPADD_16u16(a, b);
    rp.[u256 32*i] = r;
  }

  return rp;
}

fn _poly_csubq(reg ptr u16[MLKEM_N] rp) -> reg ptr u16[MLKEM_N]
{
  reg u256 r qx16;
  inline int i;
  
  qx16 = jqx16[u256 0];

  for i=0 to 16 {
    r = rp.[u256 32*i];
    r = __csubq(r, qx16);
    rp.[u256 32*i] = r;
  }

  return rp;
}

inline
fn __w256_interleave_u16(reg u256 al ah) -> reg u256, reg u256 {
 reg u256 a0 a1;

 a0  = #VPUNPCKL_16u16(al, ah);
 a1  = #VPUNPCKH_16u16(al, ah);

 return a0, a1;
}

inline
fn __w256_deinterleave_u16(reg u256 _zero a0 a1) -> reg u256, reg u256 {
  reg u256 al ah;

  al = #VPBLEND_16u16(a0,_zero,0xAA);
  ah = #VPBLEND_16u16(a1,_zero,0xAA);
  al = #VPACKUS_8u32(al, ah);
  a0 = #VPSRL_8u32(a0,16);
  a1 = #VPSRL_8u32(a1,16);
  ah = #VPACKUS_8u32(a0, a1);

  return al, ah;
}

inline
fn __mont_red(reg u256 lo hi qx16 qinvx16) -> reg u256 {
  reg u256 m;

  m  = #VPMULL_16u16(lo, qinvx16);
  m  = #VPMULH_16u16(m, qx16);
  lo = #VPSUB_16u16(hi, m);

  return lo;
}

inline
fn __wmul_16u16(reg u256 x y) -> reg u256, reg u256 {
 reg u256 xyL xyH xy0 xy1;
 xyL = #VPMULL_16u16(x, y);
 xyH = #VPMULH_16u16(x, y);
 xy0, xy1 = __w256_interleave_u16(xyL, xyH);

 return xy0, xy1;
}

inline 
fn __schoolbook16x(reg u256 are aim bre bim zeta zetaqinv qx16 qinvx16, inline int sign) -> reg u256, reg u256
{ reg u256 zaim ac0 ac1 zbd0 zbd1 ad0 ad1 bc0 bc1 x0 x1 y0 y1 _zero;

  zaim = __fqmulprecomp16x(aim, zetaqinv, zeta, qx16);

  ac0, ac1 = __wmul_16u16(are, bre);
  ad0, ad1 = __wmul_16u16(are, bim);
  bc0, bc1 = __wmul_16u16(aim, bre);
  zbd0, zbd1 = __wmul_16u16(zaim, bim);

  if (sign == 0) {
    x0 = #VPADD_8u32(ac0, zbd0);
    x1 = #VPADD_8u32(ac1, zbd1);
  } else {
    x0 = #VPSUB_8u32(ac0, zbd0);
    x1 = #VPSUB_8u32(ac1, zbd1);
  }
  y0 = #VPADD_8u32(bc0, ad0);
  y1 = #VPADD_8u32(bc1, ad1);

  _zero = #set0_256();
  x0, x1 = __w256_deinterleave_u16(_zero, x0, x1);
  y0, y1 = __w256_deinterleave_u16(_zero, y0, y1);
  x0 = __mont_red(x0, x1, qx16, qinvx16);
  y0 = __mont_red(y0, y1, qx16, qinvx16);
  return x0, y0;
}

fn _poly_basemul(reg ptr u16[MLKEM_N] rp ap bp) -> reg ptr u16[MLKEM_N]
{
  reg u256 zeta zetaqinv qx16 qinvx16 are aim bre bim;
  
  qx16    = jqx16.[u256 0];
  qinvx16 = jqinvx16.[u256 0];
  
  zetaqinv = jzetas_exp.[u256 272];
  zeta = jzetas_exp.[u256 304];

  are = ap.[u256 32*0];
  aim = ap.[u256 32*1];
  bre = bp.[u256 32*0];
  bim = bp.[u256 32*1];
  are, aim = __schoolbook16x(are, aim, bre, bim, zeta, zetaqinv, qx16, qinvx16, 0);
  rp.[u256 32*0] = are;
  rp.[u256 32*1] = aim;

  are = ap.[u256 32*2];
  aim = ap.[u256 32*3];
  bre = bp.[u256 32*2];
  bim = bp.[u256 32*3];
  are, aim = __schoolbook16x(are, aim, bre, bim, zeta, zetaqinv, qx16, qinvx16, 1);
  rp.[u256 32*2] = are;
  rp.[u256 32*3] = aim;

  zetaqinv = jzetas_exp.[u256 336];
  zeta = jzetas_exp.[u256 368];

  are = ap.[u256 32*4];
  aim = ap.[u256 32*5];
  bre = bp.[u256 32*4];
  bim = bp.[u256 32*5];
  are, aim = __schoolbook16x(are, aim, bre, bim, zeta, zetaqinv, qx16, qinvx16, 0);
  rp.[u256 32*4] = are;
  rp.[u256 32*5] = aim;

  are = ap.[u256 32*6];
  aim = ap.[u256 32*7];
  bre = bp.[u256 32*6];
  bim = bp.[u256 32*7];
  are, aim = __schoolbook16x(are, aim, bre, bim, zeta, zetaqinv, qx16, qinvx16, 1);
  rp.[u256 32*6] = are;
  rp.[u256 32*7] = aim;

  zetaqinv = jzetas_exp.[u256 664];
  zeta = jzetas_exp.[u256 696];

  are = ap.[u256 32*8];
  aim = ap.[u256 32*9];
  bre = bp.[u256 32*8];
  bim = bp.[u256 32*9];
  are, aim = __schoolbook16x(are, aim, bre, bim, zeta, zetaqinv, qx16, qinvx16, 0);
  rp.[u256 32*8] = are;
  rp.[u256 32*9] = aim;

  are = ap.[u256 32*10];
  aim = ap.[u256 32*11];
  bre = bp.[u256 32*10];
  bim = bp.[u256 32*11];
  are, aim = __schoolbook16x(are, aim, bre, bim, zeta, zetaqinv, qx16, qinvx16, 1);
  rp.[u256 32*10] = are;
  rp.[u256 32*11] = aim;

  zetaqinv = jzetas_exp.[u256 728];
  zeta = jzetas_exp.[u256 760];

  are = ap.[u256 32*12];
  aim = ap.[u256 32*13];
  bre = bp.[u256 32*12];
  bim = bp.[u256 32*13];
  are, aim = __schoolbook16x(are, aim, bre, bim, zeta, zetaqinv, qx16, qinvx16, 0);
  rp.[u256 32*12] = are;
  rp.[u256 32*13] = aim;

  are = ap.[u256 32*14];
  aim = ap.[u256 32*15];
  bre = bp.[u256 32*14];
  bim = bp.[u256 32*15];
  are, aim = __schoolbook16x(are, aim, bre, bim, zeta, zetaqinv, qx16, qinvx16, 1);
  rp.[u256 32*14] = are;
  rp.[u256 32*15] = aim;

  return rp;
}

u16 pc_shift1_s = 0x200;
u16 pc_mask_s = 0x0F;
u16 pc_shift2_s = 0x1001;
u32[8] pc_permidx_s = {0,4,1,5,2,6,3,7};

fn _poly_compress(reg u64 rp, reg ptr u16[MLKEM_N] a) -> reg ptr u16[MLKEM_N]
{
  inline int i;
  reg u256 f0 f1 f2 f3 v shift1 mask shift2 permidx;
  reg ptr u16[16] x16p;

  a = _poly_csubq(a);

  x16p = jvx16;
  v = x16p[u256 0];
  shift1 = #VPBROADCAST_16u16(pc_shift1_s);
  mask = #VPBROADCAST_16u16(pc_mask_s);
  shift2 = #VPBROADCAST_16u16(pc_shift2_s);
  permidx = pc_permidx_s[u256 0];

  for i=0 to MLKEM_N/64
  {
    f0 = a[u256 4*i];
    f1 = a[u256 4*i + 1];
    f2 = a[u256 4*i + 2];
    f3 = a[u256 4*i + 3];
    f0 = #VPMULH_16u16(f0, v);
    f1 = #VPMULH_16u16(f1, v);
    f2 = #VPMULH_16u16(f2, v);
    f3 = #VPMULH_16u16(f3, v);
    f0 = #VPMULHRS_16u16(f0, shift1);
    f1 = #VPMULHRS_16u16(f1, shift1);
    f2 = #VPMULHRS_16u16(f2, shift1);
    f3 = #VPMULHRS_16u16(f3, shift1);
    f0 = #VPAND_256(f0, mask);
    f1 = #VPAND_256(f1, mask);
    f2 = #VPAND_256(f2, mask);
    f3 = #VPAND_256(f3, mask);
    f0 = #VPACKUS_16u16(f0, f1);
    f2 = #VPACKUS_16u16(f2, f3);
    f0 = #VPMADDUBSW_256(f0, shift2);
    f2 = #VPMADDUBSW_256(f2, shift2);
    f0 = #VPACKUS_16u16(f0, f2);
    f0 = #VPERMD(permidx, f0);
    (u256)[rp + 32*i] = f0;
  }

  return a;
}

fn _poly_compress_1(reg ptr u8[MLKEM_POLYCOMPRESSEDBYTES] rp, reg ptr u16[MLKEM_N] a) -> reg ptr u8[MLKEM_POLYCOMPRESSEDBYTES], reg ptr u16[MLKEM_N]
{
  inline int i;
  reg u256 f0 f1 f2 f3 v shift1 mask shift2 permidx;
  reg ptr u16[16] x16p;

  a = _poly_csubq(a);

  x16p = jvx16;
  v = x16p[u256 0];
  shift1 = #VPBROADCAST_16u16(pc_shift1_s);
  mask = #VPBROADCAST_16u16(pc_mask_s);
  shift2 = #VPBROADCAST_16u16(pc_shift2_s);
  permidx = pc_permidx_s[u256 0];

  for i=0 to MLKEM_N/64
  {
    f0 = a[u256 4*i];
    f1 = a[u256 4*i + 1];
    f2 = a[u256 4*i + 2];
    f3 = a[u256 4*i + 3];
    f0 = #VPMULH_16u16(f0, v);
    f1 = #VPMULH_16u16(f1, v);
    f2 = #VPMULH_16u16(f2, v);
    f3 = #VPMULH_16u16(f3, v);
    f0 = #VPMULHRS_16u16(f0, shift1);
    f1 = #VPMULHRS_16u16(f1, shift1);
    f2 = #VPMULHRS_16u16(f2, shift1);
    f3 = #VPMULHRS_16u16(f3, shift1);
    f0 = #VPAND_256(f0, mask);
    f1 = #VPAND_256(f1, mask);
    f2 = #VPAND_256(f2, mask);
    f3 = #VPAND_256(f3, mask);
    f0 = #VPACKUS_16u16(f0, f1);
    f2 = #VPACKUS_16u16(f2, f3);
    f0 = #VPMADDUBSW_256(f0, shift2);
    f2 = #VPMADDUBSW_256(f2, shift2);
    f0 = #VPACKUS_16u16(f0, f2);
    f0 = #VPERMD(permidx, f0);
    rp.[u256 32*i] = f0;
  }

  return rp, a;
}

u8[32] pd_jshufbidx = {0,0,0,0,1,1,1,1,2,2,2,2,3,3,3,3,
                       4,4,4,4,5,5,5,5,6,6,6,6,7,7,7,7};
u32 pd_mask_s = 0x00F0000F;
u32 pd_shift_s = 0x800800;

fn _poly_decompress(reg ptr u16[MLKEM_N] rp, reg u64 ap) -> stack u16[MLKEM_N]
{
  inline int i;
  reg u256 f q shufbidx mask shift;
  reg u128 h;
  reg ptr u16[16] x16p;
  reg ptr u8[32] x32p;
  stack u128 sh;

  x16p = jqx16;
  q = x16p[u256 0];
  x32p = pd_jshufbidx;
  shufbidx = x32p[u256 0];
  mask = #VPBROADCAST_8u32(pd_mask_s);
  shift = #VPBROADCAST_8u32(pd_shift_s);

  f = #set0_256();

  for i=0 to MLKEM_N/16
  {
    h = (128u)(u64)[ap + 8*i];
    sh = h;
    f = #VPBROADCAST_2u128(sh);

    f = #VPSHUFB_256(f, shufbidx);
    f = #VPAND_256(f, mask);
    f = #VPMULL_16u16(f, shift);
    f = #VPMULHRS_16u16(f, q);
    rp[u256 i] = f;
  }

  return rp;
}


fn _poly_frombytes(reg ptr u16[MLKEM_N] rp, reg u64 ap) -> reg ptr u16[MLKEM_N]
{
  inline int i;
  reg u256 t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 tt mask;
  reg ptr u16[16] maskp;

  maskp = maskx16;
  mask = maskp[u256 0];

  for i=0 to 2
  {
    t0 = (u256)[ap + 192*i];
    t1 = (u256)[ap + 192*i + 32];
    t2 = (u256)[ap + 192*i + 64];
    t3 = (u256)[ap + 192*i + 96];
    t4 = (u256)[ap + 192*i + 128];
    t5 = (u256)[ap + 192*i + 160];

    tt, t3 = __shuffle8(t0, t3);
    t0, t4 = __shuffle8(t1, t4);
    t1, t5 = __shuffle8(t2, t5);

    t2, t4 = __shuffle4(tt, t4);
    tt, t1 = __shuffle4(t3, t1);
    t3, t5 = __shuffle4(t0, t5);

    t0, t1 = __shuffle2(t2, t1);
    t2, t3 = __shuffle2(t4, t3);
    t4, t5 = __shuffle2(tt, t5);

    t6, t3 = __shuffle1(t0, t3);
    t0, t4 = __shuffle1(t1, t4);
    t1, t5 = __shuffle1(t2, t5);

    t7 = #VPSRL_16u16(t6, 12);
    t8 = #VPSLL_16u16(t3, 4);
    t7 = #VPOR_256(t7, t8);
    t6 = #VPAND_256(mask, t6);
    t7 = #VPAND_256(mask, t7);

    t8 = #VPSRL_16u16(t3, 8);
    t9 = #VPSLL_16u16(t0, 8);
    t8 = #VPOR_256(t8,t9);
    t8 = #VPAND_256(mask,t8);

    t9 = #VPSRL_16u16(t0, 4);
    t9 = #VPAND_256(mask, t9);

    t10 = #VPSRL_16u16(t4, 12);
    t11 = #VPSLL_16u16(t1, 4);
    t10 = #VPOR_256(t10, t11);
    t4 = #VPAND_256(mask,t4);
    t10 = #VPAND_256(mask, t10);

    t11 = #VPSRL_16u16(t1, 8);
    tt = #VPSLL_16u16(t5, 8);
    t11 = #VPOR_256(t11, tt);
    t11 = #VPAND_256(mask, t11);

    tt = #VPSRL_16u16(t5, 4);
    tt = #VPAND_256(mask, tt);

    rp[u256 8*i] = t6;
    rp[u256 8*i + 1] = t7;
    rp[u256 8*i + 2] = t8;
    rp[u256 8*i + 3] = t9;
    rp[u256 8*i + 4] = t4;
    rp[u256 8*i + 5] = t10;
    rp[u256 8*i + 6] = t11;
    rp[u256 8*i + 7] = tt;
  }

  return rp;
}

param int DMONT   = 1353;      /* (1ULL << 32) % MLKEM_Q */

fn _poly_frommont(reg ptr u16[MLKEM_N] rp) -> reg ptr u16[MLKEM_N]
{
  reg u256 t qx16 qinvx16 dmontx16;
  inline int i;
  reg ptr u16[16] x16p;

  x16p = jqx16;
  qx16 = x16p[u256 0];
  x16p = jqinvx16;
  qinvx16 = x16p[u256 0];
  x16p = jdmontx16;
  dmontx16 = x16p[u256 0];

  for i=0 to MLKEM_N/16
  {
    t = rp[u256 i];
    t = __fqmulx16(t, dmontx16, qx16, qinvx16);
    rp[u256 i] = t;
  }

  return rp; 
}

u32[4] pfm_shift_s = {3, 2, 1, 0};
u8[16] pfm_idx_s = {0, 1, 4, 5, 8, 9, 12, 13,
                2, 3, 6, 7, 10, 11, 14, 15};

fn _poly_frommsg(reg ptr u16[MLKEM_N] rp, reg u64 ap) -> stack u16[MLKEM_N]
{
  inline int i;
  reg u256 f g0 g1 g2 g3 h0 h1 h2 h3;
  reg u256 shift idx hqs;
  reg ptr u16[16] x16p;

  x16p = hqx16_p1;
  hqs = x16p[u256 0];
  shift = #VPBROADCAST_2u128(pfm_shift_s[u128 0]);
  idx = #VPBROADCAST_2u128(pfm_idx_s[u128 0]);

  f = (u256)[ap];

  for i=0 to 4
  {
    g3 =  #VPSHUFD_256(f, 0x55*i);
    g3 = #VPSLLV_8u32(g3, shift);
    g3 = #VPSHUFB_256(g3, idx);
    g0 = #VPSLL_16u16(g3,12);
    g1 = #VPSLL_16u16(g3,8);
    g2 = #VPSLL_16u16(g3,4);
    g0 = #VPSRA_16u16(g0,15);
    g1 = #VPSRA_16u16(g1,15);
    g2 = #VPSRA_16u16(g2,15);
    g3 = #VPSRA_16u16(g3,15);
    g0 = #VPAND_256(g0,hqs);
    g1 = #VPAND_256(g1,hqs);
    g2 = #VPAND_256(g2,hqs);
    g3 = #VPAND_256(g3,hqs);
    h0 = #VPUNPCKL_4u64(g0,g1);
    h2 = #VPUNPCKH_4u64(g0,g1);
    h1 = #VPUNPCKL_4u64(g2,g3);
    h3 = #VPUNPCKH_4u64(g2,g3);
    g0 = #VPERM2I128(h0,h1,0x20);
    g2 = #VPERM2I128(h0,h1,0x31);
    g1 = #VPERM2I128(h2,h3,0x20);
    g3 = #VPERM2I128(h2,h3,0x31);
    rp[u256 2*i] = g0;
    rp[u256 2*i + 1] = g1;
    rp[u256 2*i + 8] = g2;
    rp[u256 2*i + 8 + 1] = g3;
  }

  return rp;
}


fn _poly_frommsg_1(reg ptr u16[MLKEM_N] rp, reg ptr u8[32] ap) -> stack u16[MLKEM_N]
{
  inline int i;
  reg u256 f g0 g1 g2 g3 h0 h1 h2 h3;
  reg u256 shift idx hqs;
  reg ptr u16[16] x16p;

  x16p = hqx16_p1;
  hqs = x16p[u256 0];
  shift = #VPBROADCAST_2u128(pfm_shift_s[u128 0]);
  idx = #VPBROADCAST_2u128(pfm_idx_s[u128 0]);

  f = ap[u256 0];

  for i=0 to 4
  {
    g3 = #VPSHUFD_256(f, 0x55*i);
    g3 = #VPSLLV_8u32(g3, shift);
    g3 = #VPSHUFB_256(g3, idx);
    g0 = #VPSLL_16u16(g3,12);
    g1 = #VPSLL_16u16(g3,8);
    g2 = #VPSLL_16u16(g3,4);
    g0 = #VPSRA_16u16(g0,15);
    g1 = #VPSRA_16u16(g1,15);
    g2 = #VPSRA_16u16(g2,15);
    g3 = #VPSRA_16u16(g3,15);
    g0 = #VPAND_256(g0,hqs);
    g1 = #VPAND_256(g1,hqs);
    g2 = #VPAND_256(g2,hqs);
    g3 = #VPAND_256(g3,hqs);
    h0 = #VPUNPCKL_4u64(g0,g1);
    h2 = #VPUNPCKH_4u64(g0,g1);
    h1 = #VPUNPCKL_4u64(g2,g3);
    h3 = #VPUNPCKH_4u64(g2,g3);
    g0 = #VPERM2I128(h0,h1,0x20);
    g2 = #VPERM2I128(h0,h1,0x31);
    g1 = #VPERM2I128(h2,h3,0x20);
    g3 = #VPERM2I128(h2,h3,0x31);
    rp[u256 2*i] = g0;
    rp[u256 2*i + 1] = g1;
    rp[u256 2*i + 8] = g2;
    rp[u256 2*i + 8 + 1] = g3;
  }

  return rp;
}


param int NOISE_NBLOCKS = (MLKEM_ETA1 * MLKEM_N/4 + SHAKE256_RATE - 1)/SHAKE256_RATE;

u8[32] cbd_jshufbidx = {0, 1, 2, -1, 3, 4, 5, -1, 6, 7, 8, -1, 9, 10, 11, -1,
                        4, 5, 6, -1, 7, 8, 9, -1, 10, 11, 12, -1, 13, 14, 15, -1};

inline
fn __cbd3(reg ptr u16[MLKEM_N] rp, reg ptr u8[MLKEM_ETA1*MLKEM_N/4+(MLKEM_ETA1 - 2)*8] buf) -> reg ptr u16[MLKEM_N]{
  inline int i;
  reg u256 f0 f1 f2 f3;
  reg u256 mask249 mask6DB mask07 mask70 mask3 shufbidx;
  stack u32 mask249_s mask6DB_s mask07_s mask70_s;
  stack u16 mask3_s;

  mask249_s = 0x249249;
  mask6DB_s = 0x6DB6DB;
  mask07_s = 7;
  mask70_s = (7 << 16);
  mask3_s = 3;

  mask249 = #VPBROADCAST_8u32(mask249_s);
  mask6DB = #VPBROADCAST_8u32(mask6DB_s);
  mask07  = #VPBROADCAST_8u32(mask07_s);
  mask70  = #VPBROADCAST_8u32(mask70_s);
  mask3   = #VPBROADCAST_16u16(mask3_s);
  shufbidx = cbd_jshufbidx[u256 0];

  for i=0 to MLKEM_N/32
  {
    f0 = buf.[u256 24*i];
    f0 = #VPERMQ(f0, 0x94);
    f0 = #VPSHUFB_256(f0, shufbidx);

    f1 = #VPSRL_8u32(f0, 1);
    f2 = #VPSRL_8u32(f0, 2);
    f0 = #VPAND_256(mask249, f0);
    f1 = #VPAND_256(mask249, f1);
    f2 = #VPAND_256(mask249, f2);
    f0 = #VPADD_8u32(f0, f1);
    f0 = #VPADD_8u32(f0, f2);

    f1 = #VPSRL_8u32(f0, 3);
    f0 = #VPADD_8u32(f0, mask6DB);
    f0 = #VPSUB_8u32(f0, f1);

    f1 = #VPSLL_8u32(f0, 10);
    f2 = #VPSRL_8u32(f0, 12);
    f3 = #VPSRL_8u32(f0, 2);
    f0 = #VPAND_256(f0, mask07);
    f1 = #VPAND_256(f1, mask70);
    f2 = #VPAND_256(f2, mask07);
    f3 = #VPAND_256(f3, mask70);
    f0 = #VPADD_16u16(f0, f1);
    f1 = #VPADD_16u16(f2, f3);
    f0 = #VPSUB_16u16(f0, mask3);
    f1 = #VPSUB_16u16(f1, mask3);

    f2 = #VPUNPCKL_8u32(f0, f1);
    f3 = #VPUNPCKH_8u32(f0, f1);

    f0 = #VPERM2I128(f2, f3, 0x20);
    f1 = #VPERM2I128(f2, f3, 0x31);

    rp[u256 2*i] = f0;
    rp[u256 2*i + 1] = f1;
  }

  return rp;
}


inline
fn __cbd2(reg ptr u16[MLKEM_N] rp, reg ptr u8[MLKEM_ETA2*MLKEM_N/4] buf) -> reg ptr u16[MLKEM_N]
{
  inline int i;
  reg u256 f0 f1 f2 f3;
  reg u256 mask55 mask33 mask03 mask0F;
  reg u128 t;
  stack u32 mask55_s mask33_s mask03_s mask0F_s;

  mask55_s = 0x55555555;
  mask33_s = 0x33333333;
  mask03_s = 0x03030303;
  mask0F_s = 0x0F0F0F0F;

  mask55 = #VPBROADCAST_8u32(mask55_s);
  mask33 = #VPBROADCAST_8u32(mask33_s);
  mask03 = #VPBROADCAST_8u32(mask03_s);
  mask0F = #VPBROADCAST_8u32(mask0F_s);

  for i=0 to MLKEM_N/64
  {
    f0 = buf[u256 i];

    f1 = #VPSRL_16u16(f0, 1);
    f0 = #VPAND_256(mask55, f0);
    f1 = #VPAND_256(mask55, f1);
    f0 = #VPADD_32u8(f0, f1);

    f1 = #VPSRL_16u16(f0, 2);
    f0 = #VPAND_256(mask33, f0);
    f1 = #VPAND_256(mask33, f1);
    f0 = #VPADD_32u8(f0, mask33);
    f0 = #VPSUB_32u8(f0, f1);

    f1 = #VPSRL_16u16(f0, 4);
    f0 = #VPAND_256(mask0F, f0);
    f1 = #VPAND_256(mask0F, f1);
    f0 = #VPSUB_32u8(f0, mask03);
    f1 = #VPSUB_32u8(f1, mask03);

    f2 = #VPUNPCKL_32u8(f0, f1);
    f3 = #VPUNPCKH_32u8(f0, f1);

    t = (128u)f2;
    f0 = #VPMOVSX_16u8_16u16(t);
    t = #VEXTRACTI128(f2, 1);
    f1 = #VPMOVSX_16u8_16u16(t);
    t = (128u)f3;
    f2 = #VPMOVSX_16u8_16u16(t);
    t = #VEXTRACTI128(f3, 1);
    f3 = #VPMOVSX_16u8_16u16(t);
    rp[u256 4*i] = f0;
    rp[u256 4*i + 1] = f2;
    rp[u256 4*i + 2] = f1;
    rp[u256 4*i + 3] = f3;
  }

  return rp;
}

/* buf 32 bytes longer for cbd3 (MLKEM_ETA1 == 3) */
inline
fn __poly_cbd_eta1(reg ptr u16[MLKEM_N] rp, reg ptr u8[MLKEM_ETA1*MLKEM_N/4+(MLKEM_ETA1 - 2)*8] buf) -> reg ptr u16[MLKEM_N]
{
  if(MLKEM_ETA1 == 2) { // resolved at compile-time
    rp = __cbd2(rp, buf[0:MLKEM_ETA2*MLKEM_N/4]);
  } else {
    rp = __cbd3(rp, buf);
  }

  return rp;
}

inline
fn __poly_cbd_eta2(reg ptr u16[MLKEM_N] rp, reg ptr u8[MLKEM_ETA2*MLKEM_N/4] buf) -> reg ptr u16[MLKEM_N]
{
  if(MLKEM_ETA2 == 2) {
    rp = __cbd2(rp, buf);
  }
  return rp;
}

/*
#[returnaddress="stack"]
fn _poly_getnoise(reg ptr u16[MLKEM_N] rp, reg ptr u8[MLKEM_SYMBYTES] seed, reg u8 nonce) -> reg ptr u16[MLKEM_N]
{
  inline int i;
  reg u256 f0 f1 f2 f3;
  reg u256 mask55 mask33 mask03 mask0F;
  reg u128 t;
  reg u64 t64;
  stack ptr u16[MLKEM_N] srp;
  stack u8[128] buf;
  stack u8[33] extseed;
  stack u32 mask55_s mask33_s mask03_s mask0F_s;

  mask55_s = 0x55555555;
  mask33_s = 0x33333333;
  mask03_s = 0x03030303;
  mask0F_s = 0x0F0F0F0F;

  srp = rp;

  for i=0 to MLKEM_SYMBYTES/8
  {
    t64 = seed[u64 i];
    extseed[u64 i] = t64;
  }
  extseed[MLKEM_SYMBYTES] = nonce;

  buf = _shake256_128_33(buf, extseed);

  mask55 = #VPBROADCAST_8u32(mask55_s);
  mask33 = #VPBROADCAST_8u32(mask33_s);
  mask03 = #VPBROADCAST_8u32(mask03_s);
  mask0F = #VPBROADCAST_8u32(mask0F_s);

  rp = srp;

  for i=0 to MLKEM_N/64
  {
    f0 = buf[u256 i];

    f1 = #VPSRL_16u16(f0, 1);
    f0 = #VPAND_256(mask55, f0);
    f1 = #VPAND_256(mask55, f1);
    f0 = #VPADD_32u8(f0, f1);

    f1 = #VPSRL_16u16(f0, 2);
    f0 = #VPAND_256(mask33, f0);
    f1 = #VPAND_256(mask33, f1);
    f0 = #VPADD_32u8(f0, mask33);
    f0 = #VPSUB_32u8(f0, f1);

    f1 = #VPSRL_16u16(f0, 4);
    f0 = #VPAND_256(mask0F, f0);
    f1 = #VPAND_256(mask0F, f1);
    f0 = #VPSUB_32u8(f0, mask03);
    f1 = #VPSUB_32u8(f1, mask03);

    f2 = #VPUNPCKL_32u8(f0, f1);
    f3 = #VPUNPCKH_32u8(f0, f1);

    t = (128u)f2;
    f0 = #VPMOVSX_16u8_16u16(t);
    t = #VEXTRACTI128(f2, 1);
    f1 = #VPMOVSX_16u8_16u16(t);
    t = (128u)f3;
    f2 = #VPMOVSX_16u8_16u16(t);
    t = #VEXTRACTI128(f3, 1);
    f3 = #VPMOVSX_16u8_16u16(t);
    rp[u256 4*i] = f0;
    rp[u256 4*i + 1] = f2;
    rp[u256 4*i + 2] = f1;
    rp[u256 4*i + 3] = f3;
  }

  return rp;
}
*/

inline
fn __shake256_squeezenblocks4x(reg ptr u256[25] state, reg ptr u8[NOISE_NBLOCKS * SHAKE256_RATE] buf0 buf1 buf2 buf3) -> reg ptr u256[25], reg ptr u8[NOISE_NBLOCKS*SHAKE256_RATE], reg ptr u8[NOISE_NBLOCKS*SHAKE256_RATE], reg ptr u8[NOISE_NBLOCKS*SHAKE256_RATE], reg ptr u8[NOISE_NBLOCKS*SHAKE256_RATE]
{
  inline int i;

  for i = 0 to NOISE_NBLOCKS
  {
    state, buf0[i*SHAKE256_RATE:SHAKE256_RATE], buf1[i*SHAKE256_RATE:SHAKE256_RATE], buf2[i*SHAKE256_RATE:SHAKE256_RATE], buf3[i*SHAKE256_RATE:SHAKE256_RATE] = __shake256_squeezeblock4x(state, buf0[i*SHAKE256_RATE:SHAKE256_RATE], buf1[i*SHAKE256_RATE:SHAKE256_RATE], buf2[i*SHAKE256_RATE:SHAKE256_RATE], buf3[i*SHAKE256_RATE:SHAKE256_RATE]);
  }

  return state, buf0, buf1, buf2, buf3;
}

#[returnaddress="stack"]
fn _poly_getnoise_eta1_4x(reg ptr u16[MLKEM_N] r0 r1 r2 r3, reg ptr u8[MLKEM_SYMBYTES] seed, reg u8 nonce) -> reg ptr u16[MLKEM_N], reg ptr u16[MLKEM_N], reg ptr u16[MLKEM_N], reg ptr u16[MLKEM_N]
{
  reg u256 f;
  stack u256[25] state;
  stack u8[NOISE_NBLOCKS * SHAKE256_RATE] buf0 buf1 buf2 buf3;

  f = seed[u256 0];
  buf0[u256 0] = f;
  buf1[u256 0] = f;
  buf2[u256 0] = f;
  buf3[u256 0] = f;

  buf0.[32] = nonce;
  nonce += 1;
  buf1.[32] = nonce;
  nonce += 1;
  buf2.[32] = nonce;
  nonce += 1;
  buf3.[32] = nonce;

  state = _shake256_absorb4x_33(state, buf0[0:33], buf1[0:33], buf2[0:33], buf3[0:33]);
  state, buf0, buf1, buf2, buf3 = __shake256_squeezenblocks4x(state, buf0, buf1, buf2, buf3);

  r0 = __poly_cbd_eta1(r0, buf0[0:MLKEM_ETA1*MLKEM_N/4+(MLKEM_ETA1 - 2)*8]);
  r1 = __poly_cbd_eta1(r1, buf1[0:MLKEM_ETA1*MLKEM_N/4+(MLKEM_ETA1 - 2)*8]);
  r2 = __poly_cbd_eta1(r2, buf2[0:MLKEM_ETA1*MLKEM_N/4+(MLKEM_ETA1 - 2)*8]);
  r3 = __poly_cbd_eta1(r3, buf3[0:MLKEM_ETA1*MLKEM_N/4+(MLKEM_ETA1 - 2)*8]);

  return r0, r1, r2, r3;
}

#[returnaddress="stack"]
fn _poly_getnoise_eta1122_4x(reg ptr u16[MLKEM_N] r0 r1 r2 r3, reg ptr u8[MLKEM_SYMBYTES] seed, reg u8 nonce) -> reg ptr u16[MLKEM_N], reg ptr u16[MLKEM_N], reg ptr u16[MLKEM_N], reg ptr u16[MLKEM_N]
{
  reg u256 f;
  stack u256[25] state;
  stack u8[NOISE_NBLOCKS * SHAKE256_RATE] buf0 buf1 buf2 buf3;

  f = seed[u256 0];
  buf0[u256 0] = f;
  buf1[u256 0] = f;
  buf2[u256 0] = f;
  buf3[u256 0] = f;

  buf0.[32] = nonce;
  nonce += 1;
  buf1.[32] = nonce;
  nonce += 1;
  buf2.[32] = nonce;
  nonce += 1;
  buf3.[32] = nonce;

  state = _shake256_absorb4x_33(state, buf0[0:33], buf1[0:33], buf2[0:33], buf3[0:33]);
  state, buf0, buf1, buf2, buf3 = __shake256_squeezenblocks4x(state, buf0, buf1, buf2, buf3);

  r0 = __poly_cbd_eta1(r0, buf0[0:MLKEM_ETA1*MLKEM_N/4+(MLKEM_ETA1 - 2)*8]);
  r1 = __poly_cbd_eta1(r1, buf1[0:MLKEM_ETA1*MLKEM_N/4+(MLKEM_ETA1 - 2)*8]);
  r2 = __poly_cbd_eta2(r2, buf2[0:MLKEM_ETA2*MLKEM_N/4]);
  r3 = __poly_cbd_eta2(r3, buf3[0:MLKEM_ETA2*MLKEM_N/4]);

  return r0, r1, r2, r3;
}


inline
fn __invntt___butterfly64x(reg u256 rl0 rl1 rl2 rl3 rh0 rh1 rh2 rh3 zl0 zl1 zh0 zh1 qx16) 
    -> reg u256, reg u256, reg u256, reg u256, reg u256, reg u256, reg u256, reg u256
{
  reg u256 t0 t1 t2 t3;

  t0  = #VPSUB_16u16(rl0, rh0);
  t1  = #VPSUB_16u16(rl1, rh1);
  t2  = #VPSUB_16u16(rl2, rh2);

  rl0 = #VPADD_16u16(rh0, rl0);
  rl1 = #VPADD_16u16(rh1, rl1);
  rh0 = #VPMULL_16u16(zl0, t0);

  rl2 = #VPADD_16u16(rh2, rl2);
  rh1 = #VPMULL_16u16(zl0, t1);
  t3  = #VPSUB_16u16(rl3, rh3);

  rl3 = #VPADD_16u16(rh3, rl3);
  rh2 = #VPMULL_16u16(zl1, t2);
  rh3 = #VPMULL_16u16(zl1, t3);
  
  t0  = #VPMULH_16u16(zh0, t0);
  t1  = #VPMULH_16u16(zh0, t1);

  t2  = #VPMULH_16u16(zh1, t2);
  t3  = #VPMULH_16u16(zh1, t3);

  // Reduce
  rh0  = #VPMULH_16u16(qx16, rh0);
  rh1  = #VPMULH_16u16(qx16, rh1);
  rh2  = #VPMULH_16u16(qx16, rh2);
  rh3  = #VPMULH_16u16(qx16, rh3);
  
  rh0  = #VPSUB_16u16(t0, rh0);
  rh1  = #VPSUB_16u16(t1, rh1);
  rh2  = #VPSUB_16u16(t2, rh2);
  rh3  = #VPSUB_16u16(t3, rh3);

  return rl0, rl1, rl2, rl3, rh0, rh1, rh2, rh3;
}

fn _poly_invntt(reg ptr u16[MLKEM_N] rp) -> reg ptr u16[MLKEM_N]
{
  reg u256 zeta0 zeta1 zeta2 zeta3 r0 r1 r2 r3 r4 r5 r6 r7 qx16 vx16 flox16 fhix16;
  reg ptr u16[400] zetasp;
  inline int i;

  zetasp = jzetas_inv_exp;
  qx16 = jqx16[u256 0];

  for i=0 to 2 
  {
    // level 0:
    zeta0 = zetasp.[u256 0+392*i];
    zeta1 = zetasp.[u256 64+392*i];
    zeta2 = zetasp.[u256 32+392*i];
    zeta3 = zetasp.[u256 96+392*i];

    r0 = rp.[u256 32*0+256*i];
    r1 = rp.[u256 32*1+256*i];
    r2 = rp.[u256 32*2+256*i];
    r3 = rp.[u256 32*3+256*i];
    r4 = rp.[u256 32*4+256*i];
    r5 = rp.[u256 32*5+256*i];
    r6 = rp.[u256 32*6+256*i];
    r7 = rp.[u256 32*7+256*i];

    r0, r1, r4, r5, r2, r3, r6, r7 = __invntt___butterfly64x(r0, r1, r4, r5, r2, r3, r6, r7, zeta0, zeta1, zeta2, zeta3, qx16);

    // level 1:
    vx16 = jvx16[u256 0];
    zeta0 = zetasp.[u256 128+392*i];
    zeta1 = zetasp.[u256 160+392*i];
    r0 = __red16x(r0, qx16, vx16);
    r1 = __red16x(r1, qx16, vx16);
    r4 = __red16x(r4, qx16, vx16);
    r5 = __red16x(r5, qx16, vx16);

    r0, r1, r2, r3, r4, r5, r6, r7 = __invntt___butterfly64x(r0, r1, r2, r3, r4, r5, r6, r7, zeta0, zeta0, zeta1, zeta1, qx16);
    
    r0, r1 = __shuffle1(r0, r1);
    r2, r3 = __shuffle1(r2, r3);
    r4, r5 = __shuffle1(r4, r5);
    r6, r7 = __shuffle1(r6, r7);

    // level 2:
    zeta0 = zetasp.[u256 192+392*i];
    zeta1 = zetasp.[u256 224+392*i];


    r0, r2, r4, r6, r1, r3, r5, r7 = __invntt___butterfly64x(r0, r2, r4, r6, r1, r3, r5, r7, zeta0, zeta0, zeta1, zeta1, qx16);

    r0 = __red16x(r0, qx16, vx16);

    r0, r2 = __shuffle2(r0, r2);
    r4, r6 = __shuffle2(r4, r6);
    r1, r3 = __shuffle2(r1, r3);
    r5, r7 = __shuffle2(r5, r7);

    // level 3:
    zeta0 = zetasp.[u256 256+392*i];
    zeta1 = zetasp.[u256 288+392*i];

    r0, r4, r1, r5, r2, r6, r3, r7 = __invntt___butterfly64x(r0, r4, r1, r5, r2, r6, r3, r7, zeta0, zeta0, zeta1, zeta1, qx16);

    r0 = __red16x(r0, qx16, vx16);

    r0, r4 = __shuffle4(r0, r4);
    r1, r5 = __shuffle4(r1, r5);
    r2, r6 = __shuffle4(r2, r6);
    r3, r7 = __shuffle4(r3, r7);

    // level 4:
    zeta0 = zetasp.[u256 320+392*i];
    zeta1 = zetasp.[u256 352+392*i];

    r0, r1, r2, r3, r4, r5, r6, r7 = __invntt___butterfly64x(r0, r1, r2, r3, r4, r5, r6, r7, zeta0, zeta0, zeta1, zeta1, qx16);

    r0 = __red16x(r0, qx16, vx16);

    r0, r1 = __shuffle8(r0, r1);
    r2, r3 = __shuffle8(r2, r3);
    r4, r5 = __shuffle8(r4, r5);
    r6, r7 = __shuffle8(r6, r7);

    // level 5:
    zeta0 = #VPBROADCAST_8u32(zetasp.[u32 384+392*i]);
    zeta1 = #VPBROADCAST_8u32(zetasp.[u32 388+392*i]);

    r0, r2, r4, r6, r1, r3, r5, r7 = __invntt___butterfly64x(r0, r2, r4, r6, r1, r3, r5, r7, zeta0, zeta0, zeta1, zeta1, qx16);

    r0 = __red16x(r0, qx16, vx16);

    if (i==0) {
     rp.[u256 32*0+256*i] = r0;
     rp.[u256 32*1+256*i] = r2;
     rp.[u256 32*2+256*i] = r4;
     rp.[u256 32*3+256*i] = r6;
    }
    rp.[u256 32*4+256*i] = r1;
    rp.[u256 32*5+256*i] = r3;
    rp.[u256 32*6+256*i] = r5;
    rp.[u256 32*7+256*i] = r7;
  }

  zeta0 = #VPBROADCAST_8u32(zetasp.[u32 784]);
  zeta1 = #VPBROADCAST_8u32(zetasp.[u32 788]);

  for i=0 to 2
  {
    if (i == 0) {
     r7 = r6;
     r6 = r4;
     r5 = r2;
     r4 = r0;
    } else {
     r4 = rp.[u256 32*8+128*i];
     r5 = rp.[u256 32*9+128*i];
     r6 = rp.[u256 32*10+128*i];
     r7 = rp.[u256 32*11+128*i];
    }
    r0 = rp.[u256 32*0+128*i];
    r1 = rp.[u256 32*1+128*i];
    r2 = rp.[u256 32*2+128*i];
    r3 = rp.[u256 32*3+128*i];

    r0, r1, r2, r3, r4, r5, r6, r7 = __invntt___butterfly64x(r0, r1, r2, r3, r4, r5, r6, r7, zeta0, zeta0, zeta1, zeta1, qx16);

    flox16 = jflox16[u256 0];
    fhix16 = jfhix16[u256 0];

    rp.[u256 32*8+128*i]  = r4;
    rp.[u256 32*9+128*i]  = r5;
    rp.[u256 32*10+128*i] = r6;
    rp.[u256 32*11+128*i] = r7;

    r0 = __fqmulprecomp16x(r0, flox16, fhix16, qx16);
    r1 = __fqmulprecomp16x(r1, flox16, fhix16, qx16);
    r2 = __fqmulprecomp16x(r2, flox16, fhix16, qx16);
    r3 = __fqmulprecomp16x(r3, flox16, fhix16, qx16);

    rp.[u256 32*0+128*i] = r0;
    rp.[u256 32*1+128*i] = r1;
    rp.[u256 32*2+128*i] = r2;
    rp.[u256 32*3+128*i] = r3;
  }

  return rp;
}

inline
fn __butterfly64x(reg u256 rl0 rl1 rl2 rl3 rh0 rh1 rh2 rh3 zl0 zl1 zh0 zh1 qx16) 
    -> reg u256, reg u256, reg u256, reg u256, reg u256, reg u256, reg u256, reg u256
{
  reg u256 t0 t1 t2 t3 t4 t5 t6 t7;

  t0 = #VPMULL_16u16(zl0, rh0);
  t1 = #VPMULH_16u16(zh0, rh0);
  t2 = #VPMULL_16u16(zl0, rh1);
  t3 = #VPMULH_16u16(zh0, rh1);
  t4 = #VPMULL_16u16(zl1, rh2);
  t5 = #VPMULH_16u16(zh1, rh2);
  t6 = #VPMULL_16u16(zl1, rh3);
  t7 = #VPMULH_16u16(zh1, rh3);

  t0 = #VPMULH_16u16(t0, qx16);
  t2 = #VPMULH_16u16(t2, qx16);
  t4 = #VPMULH_16u16(t4, qx16);
  t6 = #VPMULH_16u16(t6, qx16);

  //rh1 = #VPSUB_16u16(t3, rl1);
  rh1 = #VPSUB_16u16(rl1, t3);
  rl1 = #VPADD_16u16(t3, rl1);
  //rh0 = #VPSUB_16u16(t1, rl0);
  rh0 = #VPSUB_16u16(rl0, t1);
  rl0 = #VPADD_16u16(t1, rl0);
  //rh3 = #VPSUB_16u16(t7, rl3);
  rh3 = #VPSUB_16u16(rl3, t7);
  rl3 = #VPADD_16u16(t7, rl3);
  //rh2 = #VPSUB_16u16(t5, rl2);
  rh2 = #VPSUB_16u16(rl2, t5);
  rl2 = #VPADD_16u16(t5, rl2);

  rh0 = #VPADD_16u16(t0, rh0);
  //rl0 = #VPSUB_16u16(t0, rl0);
  rl0 = #VPSUB_16u16(rl0, t0);
  rh1 = #VPADD_16u16(t2, rh1);
  //rl1 = #VPSUB_16u16(t2, rl1);
  rl1 = #VPSUB_16u16(rl1, t2);
  rh2 = #VPADD_16u16(t4, rh2);
  //rl2 = #VPSUB_16u16(t4, rl2);
  rl2 = #VPSUB_16u16(rl2, t4);
  rh3 = #VPADD_16u16(t6, rh3);
  //rl3 = #VPSUB_16u16(t6, rl3);
  rl3 = #VPSUB_16u16(rl3, t6);

  return rl0, rl1, rl2, rl3, rh0, rh1, rh2, rh3;
}

fn _poly_ntt(reg ptr u16[MLKEM_N] rp) -> reg ptr u16[MLKEM_N]
{
  reg u256 zeta0 zeta1 zeta2 zeta3 r0 r1 r2 r3 r4 r5 r6 r7 qx16 vx16;
  reg ptr u16[400] zetasp;
  inline int i;

  zetasp = jzetas_exp;
  qx16 = jqx16[u256 0];

  zeta0 = #VPBROADCAST_8u32(zetasp[u32 0]);
  zeta1 = #VPBROADCAST_8u32(zetasp[u32 1]);

  r0 = rp.[u256 32*0];
  r1 = rp.[u256 32*1];
  r2 = rp.[u256 32*2];
  r3 = rp.[u256 32*3];
  r4 = rp.[u256 32*8];
  r5 = rp.[u256 32*9];
  r6 = rp.[u256 32*10];
  r7 = rp.[u256 32*11];

  r0, r1, r2, r3, r4, r5, r6, r7 = __butterfly64x(r0, r1, r2, r3, r4, r5, r6, r7, zeta0, zeta0, zeta1, zeta1, qx16);

  rp.[u256 32*0] = r0;
  rp.[u256 32*1] = r1;
  rp.[u256 32*2] = r2;
  rp.[u256 32*3] = r3;
  rp.[u256 32*8] = r4;
  rp.[u256 32*9] = r5;
  rp.[u256 32*10] = r6;
  rp.[u256 32*11] = r7;

  r0 = rp.[u256 32*4];
  r1 = rp.[u256 32*5];
  r2 = rp.[u256 32*6];
  r3 = rp.[u256 32*7];
  r4 = rp.[u256 32*12];
  r5 = rp.[u256 32*13];
  r6 = rp.[u256 32*14];
  r7 = rp.[u256 32*15];

  r0, r1, r2, r3, r4, r5, r6, r7 = __butterfly64x(r0, r1, r2, r3, r4, r5, r6, r7, zeta0, zeta0, zeta1, zeta1, qx16);

  /*
   rp.[u256 32*4] = r0;
   rp.[u256 32*5] = r1;
   rp.[u256 32*6] = r2;
   rp.[u256 32*7] = r3;
  */
  rp.[u256 32*12] = r4;
  rp.[u256 32*13] = r5;
  rp.[u256 32*14] = r6;
  rp.[u256 32*15] = r7;

  for i=0 to 2 {

    // level 1
    zeta0 = #VPBROADCAST_8u32(zetasp.[u32 8 + 392*i]);
    zeta1 = #VPBROADCAST_8u32(zetasp.[u32 12 + 392*i]);

    if ( i == 0) {
     r4 = r0;
     r5 = r1;
     r6 = r2;
     r7 = r3;
    } else {
     r4 = rp.[u256 32*4+256*i];
     r5 = rp.[u256 32*5+256*i];
     r6 = rp.[u256 32*6+256*i];
     r7 = rp.[u256 32*7+256*i];
    }
    r0 = rp.[u256 32*0+256*i];
    r1 = rp.[u256 32*1+256*i];
    r2 = rp.[u256 32*2+256*i];
    r3 = rp.[u256 32*3+256*i];

    r0, r1, r2, r3, r4, r5, r6, r7 = __butterfly64x(r0, r1, r2, r3, r4, r5, r6, r7, zeta0, zeta0, zeta1, zeta1, qx16);

    // level 2
    zeta0 = zetasp.[u256 16 + 392*i];
    zeta1 = zetasp.[u256 48 + 392*i];

    r0, r4 = __shuffle8(r0, r4);
    r1, r5 = __shuffle8(r1, r5);
    r2, r6 = __shuffle8(r2, r6);
    r3, r7 = __shuffle8(r3, r7);

    r0, r4, r1, r5, r2, r6, r3, r7 = __butterfly64x(r0, r4, r1, r5, r2, r6, r3, r7, zeta0, zeta0, zeta1, zeta1, qx16);

    // level 3
    zeta0 = zetasp.[u256 80 + 392*i];
    zeta1 = zetasp.[u256 112 + 392*i];

    r0, r2 = __shuffle4(r0, r2);
    r4, r6 = __shuffle4(r4, r6);
    r1, r3 = __shuffle4(r1, r3);
    r5, r7 = __shuffle4(r5, r7);

    r0, r2, r4, r6, r1, r3, r5, r7 = __butterfly64x(r0, r2, r4, r6, r1, r3, r5, r7, zeta0, zeta0, zeta1, zeta1, qx16);

    // level 4
    zeta0 = zetasp.[u256 144 + 392*i];
    zeta1 = zetasp.[u256 176 + 392*i];

    r0, r1 = __shuffle2(r0, r1);
    r2, r3 = __shuffle2(r2, r3);
    r4, r5 = __shuffle2(r4, r5);
    r6, r7 = __shuffle2(r6, r7);

    r0, r1, r2, r3, r4, r5, r6, r7 = __butterfly64x(r0, r1, r2, r3, r4, r5, r6, r7, zeta0, zeta0, zeta1, zeta1, qx16);

    // level 5
    zeta0 = zetasp.[u256 208 + 392*i];
    zeta1 = zetasp.[u256 240 + 392*i];

    r0, r4 = __shuffle1(r0, r4);
    r1, r5 = __shuffle1(r1, r5);
    r2, r6 = __shuffle1(r2, r6);
    r3, r7 = __shuffle1(r3, r7);

    r0, r4, r1, r5, r2, r6, r3, r7 = __butterfly64x(r0, r4, r1, r5, r2, r6, r3, r7, zeta0, zeta0, zeta1, zeta1, qx16);

    // level 6
    zeta0 = zetasp.[u256 272 + 392*i];
    zeta2 = zetasp.[u256 304 + 392*i];
    zeta1 = zetasp.[u256 336 + 392*i];
    zeta3 = zetasp.[u256 368 + 392*i];

    r0, r4, r2, r6, r1, r5, r3, r7 = __butterfly64x(r0, r4, r2, r6, r1, r5, r3, r7, zeta0, zeta1, zeta2, zeta3, qx16);

    vx16 = jvx16[u256 0];

    r0 = __red16x(r0, qx16, vx16);
    r4 = __red16x(r4, qx16, vx16);
    r2 = __red16x(r2, qx16, vx16);
    r6 = __red16x(r6, qx16, vx16);
    r1 = __red16x(r1, qx16, vx16);
    r5 = __red16x(r5, qx16, vx16);
    r3 = __red16x(r3, qx16, vx16);
    r7 = __red16x(r7, qx16, vx16);

    rp.[u256 32*0+256*i] = r0;
    rp.[u256 32*1+256*i] = r4;
    rp.[u256 32*2+256*i] = r1;
    rp.[u256 32*3+256*i] = r5;
    rp.[u256 32*4+256*i] = r2;
    rp.[u256 32*5+256*i] = r6;
    rp.[u256 32*6+256*i] = r3;
    rp.[u256 32*7+256*i] = r7;
  }

  return rp;
}

inline
fn __poly_reduce(reg ptr u16[MLKEM_N] rp) -> reg ptr u16[MLKEM_N]
{
  inline int i;
  reg u256 r qx16 vx16;
  
  qx16 = jqx16[u256 0];
  vx16 = jvx16[u256 0];

  for i=0 to 16 
  {
    r = rp.[u256 32*i];
    r = __red16x(r, qx16, vx16);
    rp.[u256 32*i] = r;
  }
  return rp;
}

fn _poly_sub(reg ptr u16[MLKEM_N] rp ap bp) -> reg ptr u16[MLKEM_N]
{
  inline int i;
  reg u256 a;
  reg u256 b;
  reg u256 r;

  for i = 0 to 16 {
    a = ap.[u256 32*i];
    b = bp.[u256 32*i];
    r = #VPSUB_16u16(a, b);
    rp.[u256 32*i] = r;
  }

  return rp;
}

fn _poly_tobytes(reg u64 rp, reg ptr u16[MLKEM_N] a) -> reg ptr u16[MLKEM_N]
{
  inline int i;
  reg u256 t0 t1 t2 t3 t4 t5 t6 t7 qx16 tt ttt;
  reg ptr u16[16] jqx16_p;

  jqx16_p = jqx16;
  qx16 = jqx16_p[u256 0];

  a = _poly_csubq(a);

  for i = 0 to 2
  {
    t0 = a[u256 8*i];
    t1 = a[u256 8*i + 1];
    t2 = a[u256 8*i + 2];
    t3 = a[u256 8*i + 3];
    t4 = a[u256 8*i + 4];
    t5 = a[u256 8*i + 5];
    t6 = a[u256 8*i + 6];
    t7 = a[u256 8*i + 7];

    tt = #VPSLL_16u16(t1, 12);
    tt |= t0;

    t0 = #VPSRL_16u16(t1, 4);
    t1 = #VPSLL_16u16(t2, 8);
    t0 |= t1;

    t1 = #VPSRL_16u16(t2, 8);
    t2 = #VPSLL_16u16(t3, 4);
    t1 |= t2;

    t2 = #VPSLL_16u16(t5, 12);
    t2 |= t4;

    t3 = #VPSRL_16u16(t5, 4);
    t4 = #VPSLL_16u16(t6, 8);
    t3 |= t4;

    t4 = #VPSRL_16u16(t6, 8);
    t5 = #VPSLL_16u16(t7, 4);
    t4 |= t5;

    ttt, t0 = __shuffle1(tt, t0);
    tt, t2 = __shuffle1(t1, t2);
    t1, t4 = __shuffle1(t3, t4);

    t3, tt= __shuffle2(ttt, tt);
    ttt, t0 = __shuffle2(t1, t0);
    t1, t4 = __shuffle2(t2, t4);

    t2, ttt = __shuffle4(t3, ttt);
    t3, tt = __shuffle4(t1, tt);
    t1, t4 = __shuffle4(t0, t4);

    t0, t3 = __shuffle8(t2, t3);
    t2, ttt = __shuffle8(t1, ttt);
    t1, t4 = __shuffle8(tt, t4);

    (u256)[rp + 192*i] = t0;
    (u256)[rp + 192*i + 32] = t2;
    (u256)[rp + 192*i + 64] = t1;
    (u256)[rp + 192*i + 96] = t3;
    (u256)[rp + 192*i + 128] = ttt;
    (u256)[rp + 192*i + 160] = t4;
  }

  return a;
}

fn _poly_tomsg(reg u64 rp, reg ptr u16[MLKEM_N] a) -> reg ptr u16[MLKEM_N]
{
  inline int i;
  reg u256 f0 f1 g0 g1 hq hhq;
  reg ptr u16[16] px16;
  reg u32 c;

  a = _poly_csubq(a);

  px16 = hqx16_m1;
  hq = px16[u256 0];

  px16 = hhqx16;
  hhq = px16[u256 0];

  for i=0 to MLKEM_N/32
  {
    f0 = a[u256 2*i];
    f1 = a[u256 2*i + 1];
    f0 = #VPSUB_16u16(hq, f0);
    f1 = #VPSUB_16u16(hq, f1);
    g0 = #VPSRA_16u16(f0, 15);
    g1 = #VPSRA_16u16(f1, 15);
    f0 = #VPXOR_256(f0, g0);
    f1 = #VPXOR_256(f1, g1);
    f0 = #VPSUB_16u16(f0, hhq);
    f1 = #VPSUB_16u16(f1, hhq);
    f0 = #VPACKSS_16u16(f0, f1);
    f0 = #VPERMQ(f0, 0xD8);
    c = #VPMOVMSKB_u256u32(f0);
    (u32)[rp+4*i] = c;
  }
  return a;
}

fn _poly_tomsg_1(reg ptr u8[MLKEM_INDCPA_MSGBYTES] rp, reg ptr u16[MLKEM_N] a) -> reg ptr u8[MLKEM_INDCPA_MSGBYTES], reg ptr u16[MLKEM_N]
{
  inline int i;
  reg u256 f0 f1 g0 g1 hq hhq;
  reg ptr u16[16] px16;
  reg u32 c;

  a = _poly_csubq(a);

  px16 = hqx16_m1;
  hq = px16[u256 0];

  px16 = hhqx16;
  hhq = px16[u256 0];

  for i=0 to MLKEM_N/32
  {
    f0 = a[u256 2*i];
    f1 = a[u256 2*i + 1];
    f0 = #VPSUB_16u16(hq, f0);
    f1 = #VPSUB_16u16(hq, f1);
    g0 = #VPSRA_16u16(f0, 15);
    g1 = #VPSRA_16u16(f1, 15);
    f0 = #VPXOR_256(f0, g0);
    f1 = #VPXOR_256(f1, g1);
    f0 = #VPSUB_16u16(f0, hhq);
    f1 = #VPSUB_16u16(f1, hhq);
    f0 = #VPACKSS_16u16(f0, f1);
    f0 = #VPERMQ(f0, 0xD8);
    c = #VPMOVMSKB_u256u32(f0);
    rp[u32 i] = c;
  }
  return rp, a;
}

inline
fn __polyvec_add2(stack u16[MLKEM_VECN] r, stack u16[MLKEM_VECN] b) -> stack u16[MLKEM_VECN]
{
  r[0:MLKEM_N]         = _poly_add2(r[0:MLKEM_N], b[0:MLKEM_N]);
  r[MLKEM_N:MLKEM_N]   = _poly_add2(r[MLKEM_N:MLKEM_N], b[MLKEM_N:MLKEM_N]);
  r[2*MLKEM_N:MLKEM_N] = _poly_add2(r[2*MLKEM_N:MLKEM_N], b[2*MLKEM_N:MLKEM_N]);

  return r;
}

inline
fn __polyvec_csubq(stack u16[MLKEM_VECN] r) -> stack u16[MLKEM_VECN]
{
  r[0:MLKEM_N] = _poly_csubq(r[0:MLKEM_N]);
  r[MLKEM_N:MLKEM_N] = _poly_csubq(r[MLKEM_N:MLKEM_N]);
  r[2*MLKEM_N:MLKEM_N] = _poly_csubq(r[2*MLKEM_N:MLKEM_N]);

  return r;
}

u32 pvd_q_s = 0x0d013404;
u8[32] pvd_shufbdidx_s = {0, 1, 1, 2, 2, 3, 3, 4,
                     5, 6, 6, 7, 7, 8, 8, 9,
                     2, 3, 3, 4, 4, 5, 5, 6,
                     7, 8, 8, 9, 9, 10, 10, 11};
u64 pvd_sllvdidx_s = 0x04;
u32 pvd_mask_s = 0x7fe01ff8;

inline
fn __polyvec_decompress(reg u64 rp) -> stack u16[MLKEM_VECN]
{
  inline int i k;
  reg u256 f q shufbidx sllvdidx mask;
  stack u16[MLKEM_VECN] r;

  q = #VPBROADCAST_8u32(pvd_q_s);
  shufbidx = pvd_shufbdidx_s[u256 0];
  sllvdidx = #VPBROADCAST_4u64(pvd_sllvdidx_s);
  mask = #VPBROADCAST_8u32(pvd_mask_s);

  for k=0 to MLKEM_K
  {
    for i=0 to MLKEM_N/16
    {
      f = (u256)[rp + 320 * k + 20 * i];
      f = #VPERMQ(f, 0x94);
      f = #VPSHUFB_256(f, shufbidx);
      f = #VPSLLV_8u32(f, sllvdidx);
      f = #VPSRL_16u16(f, 1);
      f = #VPAND_256(f, mask);
      f = #VPMULHRS_16u16(f, q);
      r[u256 16*k + i] = f;
    }
  }

  return r;
}

u16 pvc_off_s = 0x0f;
u16 pvc_shift1_s = 0x1000;
u16 pvc_mask_s = 0x03ff;
u64 pvc_shift2_s = 0x0400000104000001;
u64 pvc_sllvdidx_s = 0x0C;
u8[32] pvc_shufbidx_s = {0, 1, 2, 3, 4, 8, 9, 10, 11, 12, -1, -1, -1, -1, -1, -1,
                         9, 10, 11, 12, -1, -1, -1, -1, -1, -1, 0, 1, 2, 3, 4, 8};

inline
fn __polyvec_compress(reg u64 rp, stack u16[MLKEM_VECN] a)
{
  inline int i;
  reg u256 f0 f1 f2 v v8 off shift1 mask shift2 sllvdidx shufbidx;
  reg u128 t0 t1;
  reg ptr u16[16] x16p;

  a = __polyvec_csubq(a);

  x16p = jvx16;
  v = x16p[u256 0];
  v8 = #VPSLL_16u16(v, 3);
  off = #VPBROADCAST_16u16(pvc_off_s);
  shift1 = #VPBROADCAST_16u16(pvc_shift1_s);
  mask = #VPBROADCAST_16u16(pvc_mask_s);
  shift2 = #VPBROADCAST_4u64(pvc_shift2_s);
  sllvdidx = #VPBROADCAST_4u64(pvc_sllvdidx_s);
  shufbidx = pvc_shufbidx_s[u256 0];

  for i=0 to MLKEM_VECN/16
  {
    f0 = a[u256 i];
    f1 = #VPMULL_16u16(f0, v8);
    f2 = #VPADD_16u16(f0, off);
    f0 = #VPSLL_16u16(f0, 3);
    f0 = #VPMULH_16u16(f0, v);
    f2 = #VPSUB_16u16(f1, f2);
    f1 = #VPANDN_256(f1, f2);
    f1 = #VPSRL_16u16(f1, 15);
    f0 = #VPSUB_16u16(f0, f1);
    f0 = #VPMULHRS_16u16(f0, shift1);
    f0 = #VPAND_256(f0, mask);
    f0 = #VPMADDWD_256(f0, shift2);
    f0 = #VPSLLV_8u32(f0, sllvdidx);
    f0 = #VPSRL_4u64(f0, 12);
    f0 = #VPSHUFB_256(f0, shufbidx);
    t0 = (128u)f0;
    t1 = #VEXTRACTI128(f0, 1);
    t0 = #VPBLEND_8u16(t0, t1, 0xE0);
    (u128)[rp + 20*i] = t0;
    (u32)[rp + 20*i + 16] = #VPEXTR_32(t1, 0);
  }
}

inline
fn __polyvec_compress_1(reg ptr u8[MLKEM_POLYVECCOMPRESSEDBYTES] rp, stack u16[MLKEM_VECN] a) -> reg ptr u8[MLKEM_POLYVECCOMPRESSEDBYTES]
{
  inline int i;
  reg u256 f0 f1 f2 v v8 off shift1 mask shift2 sllvdidx shufbidx;
  reg u128 t0 t1;
  reg ptr u16[16] x16p;

  a = __polyvec_csubq(a);

  x16p = jvx16;
  v = x16p[u256 0];
  v8 = #VPSLL_16u16(v, 3);
  off = #VPBROADCAST_16u16(pvc_off_s);
  shift1 = #VPBROADCAST_16u16(pvc_shift1_s);
  mask = #VPBROADCAST_16u16(pvc_mask_s);
  shift2 = #VPBROADCAST_4u64(pvc_shift2_s);
  sllvdidx = #VPBROADCAST_4u64(pvc_sllvdidx_s);
  shufbidx = pvc_shufbidx_s[u256 0];

  for i=0 to MLKEM_VECN/16
  {
    f0 = a[u256 i];
    f1 = #VPMULL_16u16(f0, v8);
    f2 = #VPADD_16u16(f0, off);
    f0 = #VPSLL_16u16(f0, 3);
    f0 = #VPMULH_16u16(f0, v);
    f2 = #VPSUB_16u16(f1, f2);
    f1 = #VPANDN_256(f1, f2);
    f1 = #VPSRL_16u16(f1, 15);
    f0 = #VPSUB_16u16(f0, f1);
    f0 = #VPMULHRS_16u16(f0, shift1);
    f0 = #VPAND_256(f0, mask);
    f0 = #VPMADDWD_256(f0, shift2);
    f0 = #VPSLLV_8u32(f0, sllvdidx);
    f0 = #VPSRL_4u64(f0, 12);
    f0 = #VPSHUFB_256(f0, shufbidx);
    t0 = (128u)f0;
    t1 = #VEXTRACTI128(f0, 1);
    t0 = #VPBLEND_8u16(t0, t1, 0xE0);
    rp.[u128 20*i] = t0;
    rp.[u32 20*i + 16] = #VPEXTR_32(t1, 0);
  }

  return rp;
}

inline
fn __polyvec_frombytes(reg u64 ap) -> stack u16[MLKEM_VECN]
{
  stack u16[MLKEM_VECN] r;
  reg u64 pp;

  pp = ap;
  r[0:MLKEM_N] = _poly_frombytes(r[0:MLKEM_N], pp);
  pp += MLKEM_POLYBYTES;
  r[MLKEM_N:MLKEM_N] = _poly_frombytes(r[MLKEM_N:MLKEM_N], pp);
  pp += MLKEM_POLYBYTES;
  r[2*MLKEM_N:MLKEM_N] = _poly_frombytes(r[2*MLKEM_N:MLKEM_N], pp);

  return r;
}


inline
fn __polyvec_invntt(stack u16[MLKEM_VECN] r) -> stack u16[MLKEM_VECN]
{
  r[0:MLKEM_N] = _poly_invntt(r[0:MLKEM_N]);
  r[MLKEM_N:MLKEM_N] = _poly_invntt(r[MLKEM_N:MLKEM_N]);
  r[2*MLKEM_N:MLKEM_N] = _poly_invntt(r[2*MLKEM_N:MLKEM_N]);

  return r;
}


inline
fn __polyvec_ntt(stack u16[MLKEM_VECN] r) -> stack u16[MLKEM_VECN]
{
  r[0:MLKEM_N] = _poly_ntt(r[0:MLKEM_N]);
  r[MLKEM_N:MLKEM_N] = _poly_ntt(r[MLKEM_N:MLKEM_N]);
  r[2*MLKEM_N:MLKEM_N] = _poly_ntt(r[2*MLKEM_N:MLKEM_N]);

  return r;
}


inline
fn __polyvec_reduce(stack u16[MLKEM_VECN] r) -> stack u16[MLKEM_VECN]
{
  r[0:MLKEM_N] = __poly_reduce(r[0:MLKEM_N]);
  r[MLKEM_N:MLKEM_N] = __poly_reduce(r[MLKEM_N:MLKEM_N]);
  r[2*MLKEM_N:MLKEM_N] = __poly_reduce(r[2*MLKEM_N:MLKEM_N]);

  return r;
}


inline
fn __polyvec_pointwise_acc(stack u16[MLKEM_N] r, stack u16[MLKEM_VECN] a b) -> stack u16[MLKEM_N]
{
  stack u16[MLKEM_N] t;

  r = _poly_basemul(r, a[0:MLKEM_N], b[0:MLKEM_N]);
  t = _poly_basemul(t, a[MLKEM_N:MLKEM_N], b[MLKEM_N:MLKEM_N]);
  r = _poly_add2(r, t);
  t = _poly_basemul(t, a[2*MLKEM_N:MLKEM_N], b[2*MLKEM_N:MLKEM_N]);
  r = _poly_add2(r, t);

  // r = __poly_reduce(r);
  
  return r;
}


inline
fn __polyvec_tobytes(reg u64 rp, stack u16[MLKEM_VECN] a)
{
  reg u64 pp;

  pp = rp;
  a[0:MLKEM_N] = _poly_tobytes(pp, a[0:MLKEM_N]);
  pp += MLKEM_POLYBYTES;
  a[MLKEM_N:MLKEM_N] = _poly_tobytes(pp, a[MLKEM_N:MLKEM_N]);
  pp += MLKEM_POLYBYTES;
  a[2*MLKEM_N:MLKEM_N] = _poly_tobytes(pp, a[2*MLKEM_N:MLKEM_N]);
}

inline
fn __rej_uniform(stack u16[MLKEM_N] rp, reg u64 offset, stack u8[SHAKE128_RATE] buf) ->  reg u64, stack u16[MLKEM_N]
{
  reg u16 val1 val2;
  reg u16 t;
  reg u64 pos ctr;


  ctr = offset;
  pos = 0;

  while (pos < SHAKE128_RATE - 2) {
    if ctr < MLKEM_N {
      val1 = (16u)buf[pos];
      t   = (16u)buf[pos + 1];
      val2 = t;
      val2 >>= 4;
      t &= 0x0F;
      t <<= 8;
      val1 |= t;

      t   = (16u)buf[pos + 2];
      t <<= 4;
      val2 |= t;
      pos += 3;

      if val1 < MLKEM_Q {
        rp[ctr] = val1;
        ctr += 1;
      }

      if val2 < MLKEM_Q {
        if(ctr < MLKEM_N)
        {
          rp[ctr] = val2;
          ctr += 1;
        }
      }
    } else {
      pos = SHAKE128_RATE;
    }
  }

  return ctr, rp;
}

inline
fn __gen_matrix(stack u8[MLKEM_SYMBYTES] seed, reg u64 transposed) -> stack u16[MLKEM_K*MLKEM_VECN]
{
  stack u8[34] extseed;
  stack u8[SHAKE128_RATE] buf;
  stack u64[25] state;
  stack u16[MLKEM_N] poly;
  stack u16[MLKEM_K*MLKEM_VECN] r;

  reg u8 c;
  reg u16 t;
  reg u64 ctr k;
  stack u64 sctr;
  stack u64 stransposed;
  inline int j i;

  stransposed = transposed;

  for j = 0 to MLKEM_SYMBYTES
  {
    c = seed[j];
    extseed[j] = c;
  }

  for i=0 to MLKEM_K
  {
    for j = 0 to MLKEM_K
    {
      transposed = stransposed;
      if(transposed == 0)
      {
        extseed[MLKEM_SYMBYTES] = j;
        extseed[MLKEM_SYMBYTES+1] = i;
      }
      else
      {
        extseed[MLKEM_SYMBYTES] = i;
        extseed[MLKEM_SYMBYTES+1] = j;
      }

      state = _shake128_absorb34(state, extseed);

      ctr = 0;
      while (ctr < MLKEM_N)
      {
        sctr = ctr;
        state, buf = _shake128_squeezeblock(state, buf);
        ctr = sctr;
        ctr, poly  = __rej_uniform(poly, ctr, buf);
      }

      k = 0;
      reg ptr u16[MLKEM_N] rij;
      rij = r[i * MLKEM_VECN + j * MLKEM_N : MLKEM_N];
      while (k < MLKEM_N)
      {
        t = poly[(int) k];
        rij[k] = t;
        k += 1;
      }
      r[i * MLKEM_VECN + j * MLKEM_N : MLKEM_N] = rij;
    }
  }

  for i = 0 to MLKEM_K
  {
    for j = 0 to MLKEM_K
    {
      r[i*MLKEM_VECN+j*MLKEM_N:MLKEM_N] = _nttunpack(r[i*MLKEM_VECN+j*MLKEM_N:MLKEM_N]);
    }
  }

  return r;
}

inline
fn __indcpa_keypair(reg u64 pkp, reg u64 skp, reg ptr u8[MLKEM_SYMBYTES] randomnessp)
{
  stack u64 spkp sskp;
  stack u16[MLKEM_K*MLKEM_VECN] aa;
  stack u16[MLKEM_VECN] e pkpv skpv;
  stack u8[64] buf;
  stack u8[MLKEM_SYMBYTES] publicseed noiseseed;
  stack u8[32] inbuf;
  reg u64 t64;
  reg u8 nonce;
  inline int i;

  spkp = pkp;
  sskp = skp;

  for i=0 to MLKEM_SYMBYTES/8
  {
    t64 = randomnessp[u64 i];
    inbuf[u64 i] = t64;
  }

  buf = _sha3_512_32(buf, inbuf);

  for i=0 to MLKEM_SYMBYTES/8
  {
    #[declassify]
    t64 = buf[u64 i];
    publicseed[u64 i] = t64;
    t64 = buf[u64 i + MLKEM_SYMBYTES/8];
    noiseseed[u64 i] = t64;
  }

  aa = __gen_matrix(publicseed, 0);

  nonce = 0;
  skpv[0:MLKEM_N], skpv[MLKEM_N:MLKEM_N], skpv[2*MLKEM_N:MLKEM_N], e[0:MLKEM_N] = _poly_getnoise_eta1_4x(skpv[0:MLKEM_N], skpv[MLKEM_N:MLKEM_N], skpv[2*MLKEM_N:MLKEM_N], e[0:MLKEM_N], noiseseed, nonce);

  nonce = 4;
  e[MLKEM_N:MLKEM_N], e[2*MLKEM_N:MLKEM_N], pkpv[0:MLKEM_N], pkpv[MLKEM_N:MLKEM_N] = _poly_getnoise_eta1_4x(e[MLKEM_N:MLKEM_N], e[2*MLKEM_N:MLKEM_N], pkpv[0:MLKEM_N], pkpv[MLKEM_N:MLKEM_N], noiseseed, nonce);

  skpv = __polyvec_ntt(skpv);
  e    = __polyvec_ntt(e);


  for i=0 to MLKEM_K
  {
    pkpv[i*MLKEM_N:MLKEM_N] = __polyvec_pointwise_acc(pkpv[i*MLKEM_N:MLKEM_N], aa[i*MLKEM_VECN:MLKEM_VECN], skpv);
    pkpv[i*MLKEM_N:MLKEM_N] = _poly_frommont(pkpv[i*MLKEM_N:MLKEM_N]);
  }

  pkpv = __polyvec_add2(pkpv, e);
  pkpv = __polyvec_reduce(pkpv);

  pkp = spkp;
  skp = sskp;

  __polyvec_tobytes(skp, skpv);
  __polyvec_tobytes(pkp, pkpv);

  pkp += MLKEM_POLYVECBYTES;

  for i=0 to MLKEM_SYMBYTES/8
  {
    t64 = publicseed[u64 i];
    (u64)[pkp] = t64;
    pkp += 8;
  }
}

inline
fn __indcpa_enc_0(stack u64 sctp, reg ptr u8[MLKEM_INDCPA_MSGBYTES] msgp, reg u64 pkp, reg ptr u8[MLKEM_SYMBYTES] noiseseed)
{
  stack u16[MLKEM_VECN] pkpv sp ep bp;
  stack u16[MLKEM_K*MLKEM_VECN] aat;
  stack u16[MLKEM_N] k epp v;
  stack u8[MLKEM_SYMBYTES] publicseed;
  stack ptr u8[MLKEM_SYMBYTES] s_noiseseed;
  reg ptr u8[MLKEM_SYMBYTES] lnoiseseed;
  reg u64 i t64 ctp;
  reg u8 nonce;
  inline int w;

  pkpv = __polyvec_frombytes(pkp);

  i = 0;
  pkp += MLKEM_POLYVECBYTES;
  while (i < MLKEM_SYMBYTES/8)
  {
    #[declassify]
    t64 = (u64)[pkp];
    publicseed.[u64 8 * (int)i] = t64;
    pkp += 8;
    i += 1;
  }

  k = _poly_frommsg_1(k, msgp);

  s_noiseseed = noiseseed;
  aat = __gen_matrix(publicseed, 1);
  lnoiseseed = s_noiseseed;

  nonce = 0;
  sp[0:MLKEM_N], sp[MLKEM_N:MLKEM_N], sp[2*MLKEM_N:MLKEM_N], ep[0:MLKEM_N] = _poly_getnoise_eta1_4x(sp[0:MLKEM_N], sp[MLKEM_N:MLKEM_N], sp[2*MLKEM_N:MLKEM_N], ep[0:MLKEM_N], lnoiseseed, nonce);

  nonce = 4;
  ep[MLKEM_N:MLKEM_N], ep[2*MLKEM_N:MLKEM_N], epp, bp[0:MLKEM_N] = _poly_getnoise_eta1_4x(ep[MLKEM_N:MLKEM_N], ep[2*MLKEM_N:MLKEM_N], epp, bp[0:MLKEM_N], lnoiseseed, nonce);

  sp = __polyvec_ntt(sp);
    
  for w=0 to MLKEM_K
  {
    bp[w*MLKEM_N:MLKEM_N] = __polyvec_pointwise_acc(bp[w*MLKEM_N:MLKEM_N], aat[w*MLKEM_VECN:MLKEM_VECN], sp);
  }
 
  v = __polyvec_pointwise_acc(v, pkpv, sp);

  bp = __polyvec_invntt(bp);
  v = _poly_invntt(v);

  bp = __polyvec_add2(bp, ep);
  v = _poly_add2(v, epp);
  v = _poly_add2(v, k);
  bp = __polyvec_reduce(bp);
  v  = __poly_reduce(v);

  ctp = sctp;
  __polyvec_compress(ctp, bp);
  ctp += MLKEM_POLYVECCOMPRESSEDBYTES;
  v = _poly_compress(ctp, v);
}

inline
fn __indcpa_enc_1(reg ptr u8[MLKEM_INDCPA_CIPHERTEXTBYTES] ctp, reg ptr u8[MLKEM_INDCPA_MSGBYTES] msgp, reg u64 pkp, reg ptr u8[MLKEM_SYMBYTES] noiseseed) -> reg ptr u8[MLKEM_INDCPA_CIPHERTEXTBYTES]
{
  stack u16[MLKEM_VECN] pkpv sp ep bp;
  stack u16[MLKEM_K*MLKEM_VECN] aat;
  stack u16[MLKEM_N] k epp v;
  stack u8[MLKEM_SYMBYTES] publicseed;
  stack ptr u8[MLKEM_SYMBYTES] s_noiseseed;
  reg ptr u8[MLKEM_SYMBYTES] lnoiseseed;
  stack ptr u8[MLKEM_INDCPA_CIPHERTEXTBYTES] sctp;
  reg u64 i t64;
  reg u8 nonce;
  inline int w;

  sctp = ctp;

  pkpv = __polyvec_frombytes(pkp);

  i = 0;
  pkp += MLKEM_POLYVECBYTES;
  while (i < MLKEM_SYMBYTES/8)
  {
    #[declassify]
    t64 = (u64)[pkp];
    publicseed.[u64 8*(int)i] = t64;
    pkp += 8;
    i += 1;
  }

  k = _poly_frommsg_1(k, msgp);

  s_noiseseed = noiseseed;
  aat = __gen_matrix(publicseed, 1);
  lnoiseseed = s_noiseseed;

  nonce = 0;
  sp[0:MLKEM_N], sp[MLKEM_N:MLKEM_N], sp[2*MLKEM_N:MLKEM_N], ep[0:MLKEM_N] = _poly_getnoise_eta1_4x(sp[0:MLKEM_N], sp[MLKEM_N:MLKEM_N], sp[2*MLKEM_N:MLKEM_N], ep[0:MLKEM_N], lnoiseseed, nonce);

  nonce = 4;
  ep[MLKEM_N:MLKEM_N], ep[2*MLKEM_N:MLKEM_N], epp, bp[0:MLKEM_N] = _poly_getnoise_eta1_4x(ep[MLKEM_N:MLKEM_N], ep[2*MLKEM_N:MLKEM_N], epp, bp[0:MLKEM_N], lnoiseseed, nonce);

  sp = __polyvec_ntt(sp);
    
  for w=0 to MLKEM_K
  {
    bp[w*MLKEM_N:MLKEM_N] = __polyvec_pointwise_acc(bp[w*MLKEM_N:MLKEM_N], aat[w*MLKEM_VECN:MLKEM_VECN], sp);
  }
 
  v = __polyvec_pointwise_acc(v, pkpv, sp);

  bp = __polyvec_invntt(bp);
  v = _poly_invntt(v);

  bp = __polyvec_add2(bp, ep);
  v = _poly_add2(v, epp);
  v = _poly_add2(v, k);
  bp = __polyvec_reduce(bp);
  v  = __poly_reduce(v);

  ctp = sctp;
  ctp[0:MLKEM_POLYVECCOMPRESSEDBYTES] = __polyvec_compress_1(ctp[0:MLKEM_POLYVECCOMPRESSEDBYTES], bp);
  ctp[MLKEM_POLYVECCOMPRESSEDBYTES:MLKEM_POLYCOMPRESSEDBYTES], v = _poly_compress_1(ctp[MLKEM_POLYVECCOMPRESSEDBYTES:MLKEM_POLYCOMPRESSEDBYTES], v);

  return ctp;
}

inline 
fn __indcpa_dec_0(reg u64 msgp, reg u64 ctp, reg u64 skp)
{
  stack u16[MLKEM_N] t v mp;
  stack u16[MLKEM_VECN] bp skpv;

  bp = __polyvec_decompress(ctp);
  ctp += MLKEM_POLYVECCOMPRESSEDBYTES;
  v = _poly_decompress(v, ctp);

  skpv = __polyvec_frombytes(skp);
  
  bp = __polyvec_ntt(bp);
  t = __polyvec_pointwise_acc(t, skpv, bp);
  t = _poly_invntt(t);

  mp = _poly_sub(mp, v, t);
  mp = __poly_reduce(mp);
  
  mp = _poly_tomsg(msgp, mp);
}

inline
fn __indcpa_dec_1(reg ptr u8[MLKEM_INDCPA_MSGBYTES] msgp, reg u64 ctp, reg u64 skp) -> reg ptr u8[MLKEM_INDCPA_MSGBYTES]
{
  stack u16[MLKEM_N] t v mp;
  stack u16[MLKEM_VECN] bp skpv;

  bp = __polyvec_decompress(ctp);
  ctp += MLKEM_POLYVECCOMPRESSEDBYTES;
  v = _poly_decompress(v, ctp);

  skpv = __polyvec_frombytes(skp);
  
  bp = __polyvec_ntt(bp);
  t = __polyvec_pointwise_acc(t, skpv, bp);
  t = _poly_invntt(t);

  mp = _poly_sub(mp, v, t);
  mp = __poly_reduce(mp);
  
  msgp, mp = _poly_tomsg_1(msgp, mp);

  return msgp;
}

inline 
fn __verify(reg u64 ctp, reg ptr u8[MLKEM_INDCPA_CIPHERTEXTBYTES] ctpc) -> reg u64
{
  reg u256 f g h;
  reg u64 cnd t64;
  reg u8 t1 t2;
  reg bool zf;
  inline int i off;

  cnd = 0;
  t64 = 1;
  h = #set0_256();

  for i=0 to MLKEM_INDCPA_CIPHERTEXTBYTES/32
  {
    f = ctpc.[u256 32*i];
    g = (u256)[ctp + 32*i];
    f = #VPXOR_256(f, g);
    h = #VPOR_256(h, f);
  }

  _, _, _, _, zf = #VPTEST_256(h, h);

  cnd = t64 if !zf;

  off = MLKEM_INDCPA_CIPHERTEXTBYTES/32 * 32;

  for i=off to MLKEM_INDCPA_CIPHERTEXTBYTES
  {
    t1 = ctpc.[i];
    t2 = (u8)[ctp + i];
    t1 ^= t2;
    t64 = (64u)t1;
    cnd |= t64;
  }

  cnd = -cnd;
  cnd >>= 63;

  return cnd;
}

inline
fn __cmov(reg u64 dst, reg ptr u8[MLKEM_SYMBYTES] src, reg u64 cnd)
{
  reg u256 f g m;
  stack u64 scnd;
  reg u8 t1 t2 bcond;
  inline int i off;

  cnd = -cnd;
  scnd = cnd;

  m = #VPBROADCAST_4u64(scnd);

  for i=0 to MLKEM_SYMBYTES/32
  {
    f = src.[u256 32*i];
    g = (u256)[dst + 32*i];
    f = #VPBLENDVB_256(f, g, m);
    (u256)[dst + 32*i] = f;
  }

  off = MLKEM_SYMBYTES/32 * 32;

  /* fixme: unused in 768, hence untested */
  bcond = (8u)cnd;
  for i=off to MLKEM_SYMBYTES
  {
    t2 = (u8)[dst + i];
    t1 = src[i];
    t2 = t2 ^ t1;
    t2 = t2 & cnd;
    t1 ^= t2;
    (u8)[dst + i] = t1;
  }
}

inline
fn __crypto_kem_keypair_jazz(reg u64 pkp, reg u64 skp, reg ptr u8[MLKEM_SYMBYTES*2] randomnessp)
{
  stack ptr u8[MLKEM_SYMBYTES*2] s_randomnessp;
  reg ptr u8[MLKEM_SYMBYTES] randomnessp1 randomnessp2;

  stack u8[32] h_pk;
  stack u64 s_skp s_pkp;
  reg u64 t64;
  inline int i;

  s_randomnessp = randomnessp;
  s_pkp = pkp;
  s_skp = skp;

  randomnessp1 = randomnessp[0:MLKEM_SYMBYTES];
  __indcpa_keypair(pkp, skp, randomnessp1);

  skp = s_skp;
  skp += MLKEM_POLYVECBYTES;
  pkp = s_pkp;

  for i=0 to MLKEM_INDCPA_PUBLICKEYBYTES/8
  {
    (u64)[skp + 8 * i] = (u64)[pkp + 8 * i];
  }

  s_skp += MLKEM_POLYVECBYTES + MLKEM_INDCPA_PUBLICKEYBYTES;
  pkp = s_pkp;
  t64 = MLKEM_PUBLICKEYBYTES;
  h_pk = _isha3_256(h_pk, pkp, t64);
  skp = s_skp;

  for i=0 to 4
  {
    (u64)[skp + 8 * i] = h_pk[u64 i];
  }

  randomnessp = s_randomnessp;
  randomnessp2 = randomnessp[MLKEM_SYMBYTES:MLKEM_SYMBYTES];
  for i=0 to MLKEM_SYMBYTES/8
  {
    (u64)[skp + 8 * i + 32] = randomnessp2[u64 i];
  }
}

inline
fn __crypto_kem_enc_jazz(reg u64 ctp, reg u64 shkp, reg u64 pkp, reg ptr u8[MLKEM_SYMBYTES] randomnessp)
{
  inline int i;

  stack u8[MLKEM_SYMBYTES * 2] buf kr;
  stack u64 s_pkp s_ctp s_shkp;
  reg u64 t64;

  s_pkp = pkp;
  s_ctp = ctp;
  s_shkp = shkp;
  
  for i=0 to MLKEM_SYMBYTES/8
  {
    t64 = randomnessp[u64 i];
    buf[u64 i] = t64;
  }

  pkp = s_pkp;

  t64 = MLKEM_PUBLICKEYBYTES;
  buf[MLKEM_SYMBYTES:MLKEM_SYMBYTES] = _isha3_256(buf[MLKEM_SYMBYTES:MLKEM_SYMBYTES], pkp, t64);

  kr = _sha3_512_64(kr, buf);

  pkp = s_pkp;

  __indcpa_enc_0(s_ctp, buf[0:MLKEM_INDCPA_MSGBYTES], pkp, kr[MLKEM_SYMBYTES:MLKEM_SYMBYTES]);

  shkp = s_shkp;

  for i=0 to MLKEM_SYMBYTES/8
  {
    t64 = kr[u64 i];
    (u64)[shkp + 8*i] = t64;
  }
}

inline
fn __crypto_kem_dec_jazz(reg u64 shkp, reg u64 ctp, reg u64 skp)
{
  stack u8[MLKEM_INDCPA_CIPHERTEXTBYTES] ctpc;
  stack u8[2*MLKEM_SYMBYTES] kr buf;
  stack u64 s_skp s_ctp s_shkp s_cnd;
  reg u64 pkp hp zp t64 cnd;
  inline int i;

  s_shkp = shkp;
  s_ctp = ctp;

  buf[0:MLKEM_INDCPA_MSGBYTES] = __indcpa_dec_1(buf[0:MLKEM_INDCPA_MSGBYTES], ctp, skp);

  hp = skp;
  hp += 32 + (24 * MLKEM_K * MLKEM_N>>3);

  /* fixme: should loads be 256-bits long? */
  for i=0 to MLKEM_SYMBYTES/8
  {
    t64 = (u64)[hp + 8*i];
    buf.[u64 MLKEM_SYMBYTES + 8*i] = t64;
  }

  s_skp = skp;

  kr = _sha3_512_64(kr, buf);

  pkp = s_skp;
  pkp += 12 * MLKEM_K * MLKEM_N>>3;

  ctpc = __indcpa_enc_1(ctpc, buf[0:MLKEM_INDCPA_MSGBYTES], pkp, kr[MLKEM_SYMBYTES:MLKEM_SYMBYTES]);

  ctp = s_ctp;
  cnd = __verify(ctp, ctpc);
  s_cnd = cnd; /* avoidable ? */

  ctp = s_ctp;
  zp = s_skp;
  zp += 64;
  zp += 24 * MLKEM_K * MLKEM_N>>3;

  /* fixme: should this be done in memory? */
  shkp = s_shkp;
  _shake256_1120_32(shkp, zp, ctp);  

  shkp = s_shkp;
  cnd = s_cnd;
   __cmov(shkp, kr[0:MLKEM_SYMBYTES], cnd); 
}

export fn jade_kem_mlkem_mlkem768_amd64_avx2_keypair_derand(reg u64 public_key secret_key coins) -> reg u64
{
  reg u64 r;
  stack u8[MLKEM_SYMBYTES*2] randomness;
  reg ptr u8[MLKEM_SYMBYTES*2] randomnessp;
  inline int i;

  public_key = public_key;
  secret_key = secret_key;

  for i = 0 to MLKEM_SYMBYTES*2
  {
     randomness[i] = (u8)[coins + i];
  }

  randomnessp = randomness;

  __crypto_kem_keypair_jazz(public_key, secret_key, randomnessp);
  ?{}, r = #set0();
  return r;
}

export fn jade_kem_mlkem_mlkem768_amd64_avx2_enc_derand(reg u64 ciphertext shared_secret public_key coins) -> reg u64
{
  reg u64 r;
  stack u8[MLKEM_SYMBYTES] randomness;
  reg ptr u8[MLKEM_SYMBYTES] randomnessp;
  inline int i;

  ciphertext = ciphertext;
  shared_secret = shared_secret;
  public_key = public_key;

  for i = 0 to MLKEM_SYMBYTES 
  {
     randomness[i] = (u8)[coins + i];
  }

  randomnessp = randomness;

  __crypto_kem_enc_jazz(ciphertext, shared_secret, public_key, randomnessp);
  ?{}, r = #set0();
  return r;
}

export fn jade_kem_mlkem_mlkem768_amd64_avx2_keypair(reg u64 public_key secret_key) -> reg u64
{
  reg u64 r;
  stack u8[MLKEM_SYMBYTES*2] randomness;
  reg ptr u8[MLKEM_SYMBYTES*2] randomnessp;

  public_key = public_key;
  secret_key = secret_key;

  randomnessp = randomness;
  randomnessp = #randombytes(randomnessp);
  __crypto_kem_keypair_jazz(public_key, secret_key, randomnessp);
  ?{}, r = #set0();
  return r;
}

export fn jade_kem_mlkem_mlkem768_amd64_avx2_enc(reg u64 ciphertext shared_secret public_key) -> reg u64
{
  reg u64 r;
  stack u8[MLKEM_SYMBYTES] randomness;
  reg ptr u8[MLKEM_SYMBYTES] randomnessp;

  ciphertext = ciphertext;
  shared_secret = shared_secret;
  public_key = public_key;

  randomnessp = randomness;
  randomnessp = #randombytes(randomnessp);
  __crypto_kem_enc_jazz(ciphertext, shared_secret, public_key, randomnessp);
  ?{}, r = #set0();
  return r;
}

export fn jade_kem_mlkem_mlkem768_amd64_avx2_dec(reg u64 shared_secret ciphertext secret_key) -> reg u64
{
  reg u64 r;
  __crypto_kem_dec_jazz(shared_secret, ciphertext, secret_key);
  ?{}, r = #set0();
  return r;
}
